{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96593881-f7d7-447a-9d4c-c681ec167cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# This script creates census-tract based socio-economic variables over the Fema v2 database. The variables that we have here are:\n",
    "# 1. Population density\n",
    "# 2. Housing density\n",
    "# 3. Percentage of White Population   \n",
    "# 4. Percentage of Black Population    \n",
    "# 5. Percentage of Indian Population  \n",
    "# 6. Percentage of Asian Population  \n",
    "# 7. Percentage of Dual Race Population  (available only 1990 onwards)\n",
    "# 8. Median Income of the census tract\n",
    "# 9. Median Housing value of the census tract\n",
    "\n",
    "# Our main source of data are:\n",
    "# for 1990, 2000, 2010, 2011 - 2021 data: IPUMS crosswalk + IPUMS dataset\n",
    "# for 1980 data: Logan et al (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cb4eb-efb0-4bc6-b63a-dcac3e5514c1",
   "metadata": {},
   "source": [
    "## Processing 1990's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ce4b35-07fd-4355-99cc-1f95c28652e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cw90 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/IPUMS Crosswalk/nhgis_blk1990_blk2010_gj/nhgis_blk1990_blk2010_gj.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783e464f-1fa7-42ef-9508-3b7a25cdcff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GJOIN1990</th>\n",
       "      <th>GJOIN2010</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>PAREA_VIA_BLK00</th>\n",
       "      <th>len90</th>\n",
       "      <th>len10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G01000100201101A</td>\n",
       "      <td>G01000100201002004</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G01000100201101A</td>\n",
       "      <td>G01000100201002005</td>\n",
       "      <td>0.042020</td>\n",
       "      <td>0.109618</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G01000100201101A</td>\n",
       "      <td>G01000100201002006</td>\n",
       "      <td>0.262146</td>\n",
       "      <td>0.498133</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G01000100201101A</td>\n",
       "      <td>G01000100201002016</td>\n",
       "      <td>0.237187</td>\n",
       "      <td>0.218109</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G01000100201101A</td>\n",
       "      <td>G01000100201002023</td>\n",
       "      <td>0.099097</td>\n",
       "      <td>0.012864</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          GJOIN1990           GJOIN2010    WEIGHT  PAREA_VIA_BLK00  len90  \\\n",
       "0  G01000100201101A  G01000100201002004  0.000753         0.014284     16   \n",
       "1  G01000100201101A  G01000100201002005  0.042020         0.109618     16   \n",
       "2  G01000100201101A  G01000100201002006  0.262146         0.498133     16   \n",
       "3  G01000100201101A  G01000100201002016  0.237187         0.218109     16   \n",
       "4  G01000100201101A  G01000100201002023  0.099097         0.012864     16   \n",
       "\n",
       "   len10  \n",
       "0     18  \n",
       "1     18  \n",
       "2     18  \n",
       "3     18  \n",
       "4     18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw90.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df013432-bae7-464f-94b3-48aa169c28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting string length\n",
    "cw90['len90'] = cw90['GJOIN1990'].astype(str).apply(len)\n",
    "cw90['len10'] = cw90['GJOIN2010'].astype(str).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78b6625-f8c0-4141-8b3c-48420dfa9774",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Instead of recreating 2010's CBG, we instead try to identify which 1990's census tract does the 2010's CBG belong to\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Crosswalk: cw90 results\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# If census tract code = 6 digits -> 18 or 17 -> then first 14 digits\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# If census tract code = 4 digits -> 16 or 15 -> then first 12 digits\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtract90\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcw90\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGJOIN1990\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlen90\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGJOIN1990\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblockgroup10\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGJOIN2010\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m15\u001b[39m]\n\u001b[0;32m      8\u001b[0m cw90 \u001b[38;5;241m=\u001b[39m cw90[cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtract90\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9422\u001b[0m )\n\u001b[1;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Instead of recreating 2010's CBG, we instead try to identify which 1990's census tract does the 2010's CBG belong to\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Crosswalk: cw90 results\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# If census tract code = 6 digits -> 18 or 17 -> then first 14 digits\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# If census tract code = 4 digits -> 16 or 15 -> then first 12 digits\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtract90\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cw90\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGJOIN1990\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m14\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlen90\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGJOIN1990\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblockgroup10\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGJOIN2010\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m15\u001b[39m]\n\u001b[0;32m      8\u001b[0m cw90 \u001b[38;5;241m=\u001b[39m cw90[cw90[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtract90\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Instead of recreating 2010's CBG, we instead try to identify which 1990's census tract does the 2010's CBG belong to\n",
    "# Crosswalk: cw90 results\n",
    "# If census tract code = 6 digits -> 18 or 17 -> then first 14 digits\n",
    "# If census tract code = 4 digits -> 16 or 15 -> then first 12 digits\n",
    "\n",
    "cw90['tract90'] = cw90.apply(lambda row: row['GJOIN1990'][:14] if row['len90'] >= 17 else row['GJOIN1990'][:12], axis=1)\n",
    "cw90['blockgroup10'] = cw90['GJOIN2010'].str[:15]\n",
    "cw90 = cw90[cw90['tract90'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a44634-9b46-43c1-a2cf-491653971755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if blockgroup10 uniquely identify tract90 (it does not)\n",
    "bg10 = cw90.groupby('blockgroup10').size().reset_index(name='n')\n",
    "bg10_90 = cw90.groupby(['blockgroup10', 'tract90']).size().reset_index(name='n_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f152d61-1bd9-49f6-9483-ec334788c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg10_nomatchtract90 = pd.merge(bg10_90, bg10, on='blockgroup10', how='left')\n",
    "bg10_nomatchtract90['n_diff'] = bg10_nomatchtract90['n'] - bg10_nomatchtract90['n_2']\n",
    "bg10_nomatchtract90 = bg10_nomatchtract90[bg10_nomatchtract90['n_diff'] != 0]\n",
    "bg10_nomatchtract90 = bg10_nomatchtract90.groupby('blockgroup10').size().reset_index(name='n_match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f12d2-c131-43c3-944a-71cf471aa3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg10_nomatchtract90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0284306-0f7f-48ae-a717-a6742b5f9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the part that is matching multiple tract\n",
    "bg10_nomatchtract90.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/IPUMS Crosswalk/nhgis_blk1990_blk2010_gj/multi_match90.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29bbe7-553f-4aa6-81b3-dc1164612d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact tract matching\n",
    "exact_match90 = pd.merge(bg10_90, bg10, on='blockgroup10', how='left')\n",
    "exact_match90['n_diff'] = exact_match90['n'] - exact_match90['n_2']\n",
    "exact_match90 = exact_match90[exact_match90['n_diff'] == 0]\n",
    "exact_match90 = exact_match90.groupby(['blockgroup10', 'tract90']).size().reset_index(name='n')\n",
    "\n",
    "# Creating weighting factor in bcg + diff census tract level\n",
    "bg10_nomatchtract90w = pd.merge(cw90, exact_match90, on=['blockgroup10', 'tract90'], how='left')\n",
    "bg10_nomatchtract90w = bg10_nomatchtract90w.groupby(['blockgroup10', 'tract90']).agg({'WEIGHT': 'sum', 'n': 'size'}).reset_index()\n",
    "\n",
    "# Creating total weight in bcg\n",
    "bg10_totweight = bg10_nomatchtract90w.groupby('blockgroup10').agg({'WEIGHT': 'sum'}).reset_index().rename(columns={'WEIGHT': 'WEIGHT_TOTAL'})\n",
    "\n",
    "# Writing as CBG and combining match and nonmatch values\n",
    "bg10_nomatchtract90weight = pd.merge(bg10_nomatchtract90w, bg10_totweight, on='blockgroup10', how='left')\n",
    "bg10_nomatchtract90weight['weight'] = bg10_nomatchtract90weight['WEIGHT'] / bg10_nomatchtract90weight['WEIGHT_TOTAL']\n",
    "bg10_nomatchtract90weight = bg10_nomatchtract90weight[['blockgroup10', 'tract90', 'weight']].rename(columns={'tract90': 'GJOIN1990'})\n",
    "\n",
    "bg10_nomatchtract90weight.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/IPUMS Crosswalk/nhgis_blk1990_blk2010_gj/bcg_tract90_weighted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90dcc7-e28e-45f7-a055-8497bba0d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2000 dataset -- similar methods\n",
    "\n",
    "cw00 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/IPUMS Crosswalk/nhgis_blk2000_blk2010_gj/nhgis_blk2000_blk2010_gj.csv\")\n",
    "\n",
    "# Getting string length\n",
    "cw00['len00'] = cw00['GJOIN2000'].astype(str).apply(len)\n",
    "cw00['len10'] = cw00['GJOIN2010'].astype(str).apply(len)\n",
    "\n",
    "# Identifying census tracts and block groups\n",
    "cw00['tract00'] = cw00['GJOIN2000'].str[:14]\n",
    "cw00['blockgroup10'] = cw00['GJOIN2010'].str[:15]\n",
    "cw00 = cw00[cw00['tract00'] != \"\"]\n",
    "\n",
    "# Total weight in cbg\n",
    "bg10 = cw00.groupby('blockgroup10')['WEIGHT'].sum().reset_index(name='weight_total')\n",
    "\n",
    "# Total weight in cbg + census tract\n",
    "bg10_00 = cw00.groupby(['blockgroup10', 'tract00'])['WEIGHT'].sum().reset_index(name='weight_b')\n",
    "\n",
    "# Calculating weight portion\n",
    "bg10_nomatchtract00weight = pd.merge(bg10_00, bg10, on='blockgroup10')\n",
    "bg10_nomatchtract00weight['weight'] = bg10_nomatchtract00weight['weight_b'] / bg10_nomatchtract00weight['weight_total']\n",
    "bg10_nomatchtract00weight = bg10_nomatchtract00weight[['blockgroup10', 'tract00', 'weight']].rename(columns={'tract00': 'GJOIN2000'})\n",
    "\n",
    "bg10_nomatchtract00weight.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/IPUMS Crosswalk/nhgis_blk2000_blk2010_gj/bcg_tract00_weighted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87b31c-a85f-42b9-b318-771c8afa1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2020 dataset\n",
    "cw20 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/IPUMS Crosswalk/nhgis_blk2020_blk2010_gj/nhgis_blk2020_blk2010_gj.csv\")\n",
    "\n",
    "# Identifying census tracts and block groups\n",
    "cw20['tract20'] = cw20['GJOIN2020'].str[:14]\n",
    "cw20['blockgroup10'] = cw20['GJOIN2010'].str[:15]\n",
    "cw20 = cw20[cw20['tract20'] != \"\"]\n",
    "cw20 = cw20.drop(columns=['GJOIN2020']).rename(columns={'tract20': 'GJOIN2020'})\n",
    "\n",
    "# Total weight in cbg\n",
    "bg10 = cw20.groupby('blockgroup10')['WEIGHT'].sum().reset_index(name='weight_total')\n",
    "\n",
    "# Total weight in cbg + census tract\n",
    "bg10_20 = cw20.groupby(['blockgroup10', 'GJOIN2020'])['WEIGHT'].sum().reset_index(name='weight_b')\n",
    "\n",
    "# Calculating weight portion\n",
    "bg20_nomatchtract00weight = pd.merge(bg10_20, bg10, on='blockgroup10')\n",
    "bg20_nomatchtract00weight['weight'] = bg20_nomatchtract00weight['weight_b'] / bg20_nomatchtract00weight['weight_total']\n",
    "bg20_nomatchtract00weight = bg20_nomatchtract00weight[['blockgroup10', 'GJOIN2020', 'weight']]\n",
    "\n",
    "bg20_nomatchtract00weight.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/IPUMS Crosswalk/bcg_tract20_weighted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9861cff-3440-4f90-bb93-defa6ebd1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1980 dataset -> Using tract TS from Trent\n",
    "# Lowest unit in GSJOIN2010 -> Census tract instead of census block\n",
    "\n",
    "# Reading datasets and initial filtering commented out in the original script\n",
    "# cwts = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/nhgis0009_csv/nhgis0009_ts_nominal_tract.csv\")\n",
    "\n",
    "# Trying to combine the data with the original dataset\n",
    "data2 = pd.read_parquet(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/july_24_flood_data.parquet.gzip\")\n",
    "data2 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/FimaNfipClaims.csv\", dtype={'censusBlockGroupFips': str})\n",
    "\n",
    "# All CBG data: 1980, 1990, 2000, 2020\n",
    "\n",
    "def gjoin_splitter(var_name):\n",
    "    a = var_name[1:3]\n",
    "    b = var_name[4:7]\n",
    "    c = var_name[8:15]\n",
    "    return \"\".join([a, b, c])\n",
    "\n",
    "# Example call to the function\n",
    "print(gjoin_splitter(\"ancadasdadasdaadasds\"))\n",
    "\n",
    "# Creating census block group data and calculating match percentages is outlined but not fully implementable without additional context or data.\n",
    "\n",
    "# Reading additional datasets for comparison\n",
    "all90 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/bcg_tract90_weighted.csv\")\n",
    "all00 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/bcg_tract00_weighted.csv\")\n",
    "all20 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/bcg_tract20_weighted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec97b8-291a-4842-ba50-f62caa68aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV and initial data processing for 1990 dataset\n",
    "cwts = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/nhgis0009_csv/nhgis0009_ts_nominal_tract.csv\")\n",
    "\n",
    "# Calculating empty counts\n",
    "empty_counts = (cwts == \"\").sum()\n",
    "\n",
    "# Processing data for 1980, noting the lack of a direct crosswalk\n",
    "all80 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/crosswalk_1980_2010.csv\", \n",
    "                    dtype={'trtid80': str, 'trtid10': str})\n",
    "all80['nchar80'] = all80['trtid80'].apply(len)\n",
    "all80['nchar10'] = all80['trtid10'].apply(len)\n",
    "\n",
    "# Adding FIPS codes and merging for 1980 data\n",
    "all80 = all80.assign(\n",
    "    state_fips=all80['trtid10'].str[:2],\n",
    "    county_fips=all80['trtid10'].str[2:5],\n",
    "    census_block_group=all80['trtid10'].str[5:11]\n",
    ").assign(\n",
    "    censusTract10=lambda x: \"G\" + x['state_fips'] + \"0\" + x['county_fips'] + \"0\" + x['census_block_group'],\n",
    "    GJOIN1980=lambda x: \"G\" + x['state_fips'] + \"0\" + x['county_fips'] + \"0\" + x['census_block_group']\n",
    ").loc[:, ['GJOIN1980', 'censusTract10', 'weight']]\n",
    "\n",
    "# Summarizing weights for 1980 data\n",
    "weight_summary_80 = all80.groupby('censusTract10')['weight'].sum().reset_index(name='tot_weight')\n",
    "\n",
    "# Processing and transforming data for 1990\n",
    "cwts_90 = cwts.query(\"GJOIN1990 != ''\").assign(\n",
    "    population=lambda x: x['AV0AA1990'],\n",
    "    populationWhite=lambda x: x['B18AA1990'],\n",
    "    populationBlack=lambda x: x['B18AB1990'],\n",
    "    populationIndian=lambda x: x['B18AC1990'],\n",
    "    populationAsian=lambda x: x['B18AD1990'],\n",
    "    housingUrban=lambda x: x['AZ7AA1990'],\n",
    "    housingRural=lambda x: x['AZ7AD1990'],\n",
    "    medianIncome=lambda x: x['B79AA1990'],\n",
    "    housingTotal=lambda x: x['A41AA1990']\n",
    ").assign(\n",
    "    exist=1\n",
    ")\n",
    "\n",
    "# Converting population figures into percentages for 1990\n",
    "cwts_90 = cwts_90.assign(\n",
    "    population_tot=lambda x: x[['populationWhite', 'populationBlack', 'populationIndian', 'populationAsian']].sum(axis=1)\n",
    ").assign(\n",
    "    percpopulationWhite=lambda x: x['populationWhite'] / x['population_tot'],\n",
    "    percpopulationBlack=lambda x: x['populationBlack'] / x['population_tot'],\n",
    "    percpopulationIndian=lambda x: x['populationIndian'] / x['population_tot'],\n",
    "    percpopulationAsian=lambda x: x['populationAsian'] / x['population_tot']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd62c2b-0bbe-4580-965a-2ecce65ebe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO!\n",
    "# values in 2010 onwards (need some flip-join maybe)\n",
    "\n",
    "# Also to do: Obtain values of LAND_AREA and median house value in these areas:\n",
    "\n",
    "# Conversion factor\n",
    "sqmt_to_sqmiles = 3.86102e-7\n",
    "\n",
    "# Processing data for 1990\n",
    "df90 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/nhgis0009_csv/nhgis0009_ds120_1990_tract.csv\")\n",
    "df90 = df90[['GISJOIN', 'AREALAND', 'EST001']].rename(columns={'AREALAND': 'LAND_AREA', 'EST001': 'Value'})\n",
    "df90['LAND_AREA'] = df90['LAND_AREA'] * sqmt_to_sqmiles\n",
    "\n",
    "# Processing data for 2000\n",
    "df00 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/nhgis0009_csv/nhgis0009_ds151_2000_tract.csv\")\n",
    "df00 = df00[['GISJOIN', 'AREALAND', 'GB7001']].rename(columns={'AREALAND': 'LAND_AREA', 'GB7001': 'Value'})\n",
    "df00['LAND_AREA'] = df00['LAND_AREA'] * sqmt_to_sqmiles\n",
    "\n",
    "# Processing data for 2010\n",
    "df10 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/pdb2012bgv9_us.csv\", dtype={'GIDBG': str})\n",
    "df10 = df10[['GIDBG', 'LAND_AREA', 'Med_house_val_tr_ACS_06_10']].rename(columns={'Med_house_val_tr_ACS_06_10': 'Value'})\n",
    "df10['Value'] = pd.to_numeric(df10['Value'], errors='coerce')  # Assuming parse_number equivalent\n",
    "df10['gidbg'] = df10['GIDBG'].str[:11]\n",
    "df10 = df10.assign(\n",
    "    state_fips=df10['gidbg'].str[:2],\n",
    "    county_fips=df10['gidbg'].str[2:5],\n",
    "    census_tract=df10['gidbg'].str[5:11]\n",
    ").assign(GISJOIN=lambda x: \"G\" + x['state_fips'] + \"0\" + x['county_fips'] + \"0\" + x['census_tract'])\n",
    "df10 = df10.groupby('GISJOIN').agg({'LAND_AREA': 'sum', 'Value': 'mean'}).reset_index()\n",
    "\n",
    "# Processing data for 2020\n",
    "df20 = pd.read_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/pdb2022tr.csv\", dtype={'GIDTR': str})\n",
    "df20 = df20[['GIDTR', 'LAND_AREA', 'Med_House_Value_ACS_16_20']].rename(columns={'Med_House_Value_ACS_16_20': 'Value'})\n",
    "df20['Value'] = pd.to_numeric(df20['Value'], errors='coerce')  # Assuming parse_number equivalent\n",
    "df20 = df20.assign(\n",
    "    state_fips=df20['GIDTR'].str[:2],\n",
    "    county_fips=df20['GIDTR'].str[2:5],\n",
    "    census_tract=df20['GIDTR'].str[5:11]\n",
    ").assign(GISJOIN=lambda x: \"G\" + x['state_fips'] + \"0\" + x['county_fips'] + \"0\" + x['census_tract'])\n",
    "df20 = df20[['GISJOIN', 'LAND_AREA', 'Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f79ab-c476-43d7-8197-cd04ce618efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all90, df90, all00, df00, and cwts dataframes are already loaded as per previous instructions.\n",
    "\n",
    "# Conversion for square meters to square miles\n",
    "sqmt_to_sqmiles = 3.86102e-7\n",
    "\n",
    "# Correcting the name convention for all\n",
    "all90.rename(columns={'tract90': 'GJOIN1990'}, inplace=True)\n",
    "all90 = all90[['blockgroup10', 'GJOIN1990', 'weight']]\n",
    "\n",
    "df90.rename(columns={'GISJOIN': 'GJOIN1990'}, inplace=True)\n",
    "\n",
    "all00.rename(columns={'tract00': 'GJOIN2000'}, inplace=True)\n",
    "all00 = all00[['blockgroup10', 'GJOIN2000', 'weight']]\n",
    "\n",
    "df00.rename(columns={'GISJOIN': 'GJOIN2000'}, inplace=True)\n",
    "\n",
    "# Writing 90's data\n",
    "full_90_data = pd.merge(all90, df90, on='GJOIN1990', how='left')\n",
    "full_90_data = pd.merge(full_90_data, cwts_90, on='GJOIN1990', how='left')\n",
    "full_90_data['Year'] = 1990\n",
    "full_90_data.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/all90_data.csv\", index=False)\n",
    "\n",
    "# Writing 00's data\n",
    "full_00_data = pd.merge(all00, df00, on='GJOIN2000', how='left')\n",
    "full_00_data = pd.merge(full_00_data, cwts_00, on='GJOIN2000', how='left')\n",
    "full_00_data['Year'] = 2000\n",
    "full_00_data.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nghis census tract/nhgisCensusTract_csv/all00_data.csv\", index=False)\n",
    "\n",
    "# For 2010's data, assuming cwts is already loaded and prepared\n",
    "# The R script uses tidyverse functions to rename columns based on patterns and pivot_longer which are not directly available in pandas. We will approximate these steps.\n",
    "\n",
    "# Renaming columns based on patterns (assuming cwts is already prepared similarly to cwts_90 and cwts_00)\n",
    "rename_columns = {col: col.split('_')[0] + col[-4:] for col in cwts.columns if '2010' in col}\n",
    "cwts.rename(columns=rename_columns, inplace=True)\n",
    "\n",
    "# Pivoting longer - converting wide format to long format (an approximate approach using pd.melt, since pivot_longer is not a direct pandas function)\n",
    "cwts_melted = cwts.melt(id_vars=['GJOIN2010'], var_name='variable', value_name='value')\n",
    "cwts_melted['Year'] = cwts_melted['variable'].apply(lambda x: int(x[-4:]))\n",
    "\n",
    "# Assuming df10 is already loaded and prepared, and joining data for 2010's\n",
    "full_10_data = pd.merge(cwts_melted, df10, left_on='GJOIN2010', right_on='GISJOIN', how='left')\n",
    "full_10_data = full_10_data[full_10_data['Year'] < 2020]\n",
    "full_10_data.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nhgisCensusTract_csv/all10_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91736583-bd61-4ad7-a097-93016c2dd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating missing values\n",
    "missing_values = cwts.isna().sum()\n",
    "\n",
    "# Filtering and selecting for 2020\n",
    "cwts20 = cwts[cwts['GJOIN2020'] != \"\"].filter(regex='(AV|B18|AZ7|B79|A41).*5|GJOIN2020')\n",
    "\n",
    "# Dynamically renaming columns based on patterns\n",
    "pattern_subs = {\n",
    "    'AV0AA': 'population_',\n",
    "    'B18AA': 'populationWhite_',\n",
    "    'B18AB': 'populationBlack_',\n",
    "    'B18AC': 'populationIndian_',\n",
    "    'B18AD': 'populationAsian_',\n",
    "    'B18AE': 'populationDualRace_',\n",
    "    'AZ7AA': 'housingUrban_',\n",
    "    'AZ7AD': 'housingRural_',\n",
    "    'B79AA': 'medianIncome_',\n",
    "    'A41AA': 'housingTotal_'\n",
    "}\n",
    "\n",
    "for pattern, replacement in pattern_subs.items():\n",
    "    cwts20.columns = [re.sub(pattern + '(\\d+)', replacement + '\\\\1', col) for col in cwts20.columns]\n",
    "\n",
    "# Pivoting longer - converting wide format to long format. We use pandas.melt() as an approximate solution.\n",
    "cwts20_melted = cwts20.melt(id_vars=['GJOIN2020'], var_name='variable', value_name='value')\n",
    "cwts20_melted['Group'] = cwts20_melted['variable'].str.extract('(\\d+)$').astype(int)\n",
    "cwts20_melted['Year'] = (cwts20_melted['Group'] - 5) / 10 + 2000\n",
    "cwts20_melted.rename(columns={'GJOIN2020': 'GISJOIN'}, inplace=True)\n",
    "\n",
    "# Assuming all20 dataframe is already prepared and df20 loaded from previous steps\n",
    "all21 = all20.copy()\n",
    "\n",
    "cwts_20_only = cwts20_melted[cwts20_melted['Year'] == 2020]\n",
    "cwts_21_only = cwts20_melted[cwts20_melted['Year'] == 2021]\n",
    "\n",
    "# Merging and preparing the final data for 2020\n",
    "full_20_data = pd.merge(all20.rename(columns={'tract20': 'GISJOIN'}), cwts_20_only, on='GISJOIN', how='left')\n",
    "full_20_data = pd.merge(full_20_data, df20, on='GISJOIN', how='left').drop(columns=['Group'])\n",
    "\n",
    "full_20_data.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nhgisCensusTract_csv/all20_data.csv\", index=False)\n",
    "\n",
    "# Preparing the final data for 2021\n",
    "full_21_data = pd.merge(all21.rename(columns={'tract20': 'GISJOIN'}), cwts_21_only, on='GISJOIN', how='left')\n",
    "full_21_data = pd.merge(full_21_data, df20, on='GISJOIN', how='left')\n",
    "\n",
    "full_21_data.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nhgisCensusTract_csv/all21_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a9319-45be-4d65-996c-f85e848b5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA data processing for different decades\n",
    "fema90 = data2[(data2['yearOfLoss'] >= 1990) & (data2['yearOfLoss'] < 2000)]\n",
    "fema00 = data2[(data2['yearOfLoss'] >= 2000) & (data2['yearOfLoss'] < 2010)]\n",
    "fema20 = data2[(data2['yearOfLoss'] >= 2020) & (data2['yearOfLoss'] < 2029)]\n",
    "\n",
    "# Assuming all90, all00, and all20 dataframes are already loaded as per previous instructions\n",
    "\n",
    "# Identifying non-matching weights for different datasets\n",
    "weight_nonmatch90 = all90[all90['weight'] != 1]\n",
    "non_match90 = weight_nonmatch90.groupby('blockgroup10').agg(n=('weight', 'size'), tot_weight=('weight', 'sum')).reset_index()\n",
    "\n",
    "weight_nonmatch00 = all00[all00['weight'] != 1]\n",
    "non_match00 = weight_nonmatch00.groupby('blockgroup10').agg(n=('weight', 'size'), tot_weight=('weight', 'sum')).reset_index()\n",
    "\n",
    "weight_nonmatch20 = all20[all20['weight'] != 1]\n",
    "non_match20 = weight_nonmatch20.groupby('blockgroup10').agg(n=('weight', 'size'), tot_weight=('weight', 'sum')).reset_index()\n",
    "\n",
    "# Adding GIS mode to FEMA datasets and joining with non-match data\n",
    "def add_gis_mode(df):\n",
    "    df['nchar'] = df['censusBlockGroupFips'].apply(len)\n",
    "    df['state_fips'] = df['censusBlockGroupFips'].str[:2]\n",
    "    df['county_fips'] = df['censusBlockGroupFips'].str[2:5]\n",
    "    df['census_block_group'] = df['censusBlockGroupFips'].str[5:12]\n",
    "    df['blockgroup10'] = \"G\" + df['state_fips'] + \"0\" + df['county_fips'] + \"0\" + df['census_block_group']\n",
    "    return df\n",
    "\n",
    "fema90_GIS = add_gis_mode(fema90)\n",
    "fema00_GIS = add_gis_mode(fema00)\n",
    "fema20_GIS = add_gis_mode(fema20)\n",
    "\n",
    "fema90_GIS = pd.merge(fema90_GIS, non_match90, on='blockgroup10', how='left').dropna(subset=['tot_weight'])\n",
    "fema00_GIS = pd.merge(fema00_GIS, non_match00, on='blockgroup10', how='left').dropna(subset=['tot_weight'])\n",
    "fema20_GIS = pd.merge(fema20_GIS, non_match20, on='blockgroup10', how='left').dropna(subset=['tot_weight'])\n",
    "\n",
    "# Exporting to CSV\n",
    "fema90_GIS.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nhgisCensusTract_csv/multi_tract90_confirm.csv\", index=False)\n",
    "fema00_GIS.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nhgisCensusTract_csv/multi_tract00_confirm.csv\", index=False)\n",
    "fema20_GIS.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nhgisCensusTract_csv/multi_tract20_confirm.csv\", index=False)\n",
    "\n",
    "# The section for 1980's data and the crosswalk utilization are noted but not directly implemented due to the nature of the instructions and the need for specific datasets and additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa3d8a-7db5-4c79-bdcc-363baeb5e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all80, all80sam, and area dataframes are loaded as per previous instructions\n",
    "\n",
    "# Renaming and calculating population percentages for the 1980 dataset\n",
    "all80_new = all80.rename(columns={\n",
    "    'NHWHT80': 'populationWhite',\n",
    "    'NTV80': 'populationIndian',\n",
    "    'NHBLK80': 'populationBlack',\n",
    "    'ASIAN80': 'populationAsian',\n",
    "    'POP80': 'population',\n",
    "    'MHMVAL80': 'Value',\n",
    "    'TRTID10': 'trtid10'\n",
    "})\n",
    "all80_new['population_tot'] = all80_new[['populationWhite', 'populationBlack', 'populationIndian', 'populationAsian']].sum(axis=1)\n",
    "all80_new['population_diff'] = all80_new['population_tot'] - all80_new['population']  # Note on potential discrepancies\n",
    "all80_new['percpopulationWhite'] = all80_new['populationWhite'] / all80_new['population_tot']\n",
    "all80_new['percpopulationBlack'] = all80_new['populationBlack'] / all80_new['population_tot']\n",
    "all80_new['percpopulationIndian'] = all80_new['populationIndian'] / all80_new['population_tot']\n",
    "all80_new['percpopulationAsian'] = all80_new['populationAsian'] / all80_new['population_tot']\n",
    "all80_new['trtid10'] = all80_new['trtid10'].str.zfill(11)  # Padding trtid10 to ensure 11 characters\n",
    "\n",
    "# Processing all80sam data\n",
    "all80sam_new = all80sam.rename(columns={'hinc80': 'medianIncome', 'hh80': 'house'})\n",
    "all80sam_new['Year'] = 1980\n",
    "all80sam_new['exist'] = 1\n",
    "all80sam_new['trtid10'] = all80sam_new['trtid10'].str.zfill(11)  # Padding trtid10\n",
    "\n",
    "# Assuming 'area' dataframe is loaded from the area data mentioned in the comment\n",
    "area['GIDBG'] = area['GIDBG'].apply(lambda x: x.zfill(12))\n",
    "area['trtid10'] = area['GIDBG'].str[:11]\n",
    "area = area.groupby('trtid10')['LAND_AREA'].sum().reset_index()\n",
    "\n",
    "# Joining datasets\n",
    "all80_new = pd.merge(all80_new, all80sam_new, on='trtid10', how='left')\n",
    "all80_new = pd.merge(all80_new, area, on='trtid10', how='left')\n",
    "\n",
    "# Final adjustment and selection for each year\n",
    "all80_final = all80_new.assign(\n",
    "    populationDensity=all80_new['population'] / all80_new['LAND_AREA'],\n",
    "    housingDensity=all80_new['house'] / all80_new['LAND_AREA']\n",
    ").filter(items=[\n",
    "    'trtid10', 'percpopulationWhite', 'percpopulationBlack', 'percpopulationIndian', 'percpopulationAsian',\n",
    "    'populationDensity', 'housingDensity', 'medianIncome', 'Year'\n",
    "])\n",
    "\n",
    "all80_final.to_csv(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/nhgisCensusTract_csv/all80_data.csv\", index=False)\n",
    "\n",
    "# Adjusting LAND_AREA for 1990 data in full_90_data\n",
    "# Assuming full_90_data dataframe is loaded as per previous instructions\n",
    "full_90_data['LAND_AREA'] *= 1000  # Conversion if necessary\n",
    "full_90_data['populationDensity'] = full_90_data['population'] / full_90_data['LAND_AREA']\n",
    "full_90_data['housingDensity'] = full_90_data['housingTotal'] / full_90_data['LAND_AREA']\n",
    "# Selecting and renaming for final 1990 dataset preparation\n",
    "all90_final = full_90_data[['blockgroup10', 'GJOIN1990', 'percpopulationWhite', 'percpopulationBlack', \n",
    "                            'percpopulationIndian', 'percpopulationAsian', 'populationDensity', \n",
    "                            'housingDensity', 'medianIncome', 'Year']]\n",
    "\n",
    "# Assuming fema80 dataset is loaded as per previous instructions\n",
    "fema80new = fema80.assign(cbgexist=fema80['censusBlockGroupFips'].apply(len) > 0)\n",
    "fema80new['trtid10'] = fema80new['censusBlockGroupFips'].str[:11]\n",
    "\n",
    "# Final join for FEMA data with all80 data\n",
    "final_data = pd.merge(fema80new, all80_final, on='trtid10', how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
