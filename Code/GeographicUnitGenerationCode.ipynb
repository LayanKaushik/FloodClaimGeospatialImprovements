{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a2dfd-51ff-419f-bd67-f9d6b94d0377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "\n",
    "#Packages\n",
    "import matplotlib.ticker as mtick\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import pygris\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4539c2-d0a2-4cce-8507-4176e5aaa9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/FimaNfipClaims.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d1d8f-396a-4e90-b8e3-418d10076245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941ee3c-82c1-4baf-86b0-3a5d7983ecdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fill na values with 0\n",
    "\n",
    "df_geographic_unique = df[['state', 'reportedZipCode', 'countyCode', 'censusTract', 'censusBlockGroupFips', 'latitude', 'longitude', 'yearOfLoss']].drop_duplicates()\n",
    "\n",
    "df_geographic_unique['reportedZipCode'] = df_geographic_unique['reportedZipCode'].fillna(0)\n",
    "df_geographic_unique['countyCode'] = df_geographic_unique['countyCode'].fillna(0)\n",
    "df_geographic_unique['censusTract'] = df_geographic_unique['censusTract'].fillna(0)\n",
    "df_geographic_unique['censusBlockGroupFips'] = df_geographic_unique['censusBlockGroupFips'].fillna(0)\n",
    "df_geographic_unique['latitude'] = df_geographic_unique['latitude'].fillna(0)\n",
    "df_geographic_unique['longitude'] = df_geographic_unique['longitude'].fillna(0)\n",
    "\n",
    "df_geographic_unique = df_geographic_unique.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f731d-1636-4598-94bc-5ea5d9f5dc7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert the units to string to use mapping and fix the lenght of each units\n",
    "\n",
    "df_geographic_unique['reportedZipCode'] = df_geographic_unique['reportedZipCode'].astype(int).astype(str)\n",
    "df_geographic_unique['reportedZipCode'] = [zipcode.zfill(5) for zipcode in df_geographic_unique['reportedZipCode']]\n",
    "\n",
    "df_geographic_unique['censusBlockGroupFips'] = [str(int(float(i))) for i in df_geographic_unique['censusBlockGroupFips']]\n",
    "df_geographic_unique['censusBlockGroupFips'] = [censusBG.zfill(12) for censusBG in df_geographic_unique['censusBlockGroupFips']]\n",
    "\n",
    "df_geographic_unique['countyCode'] = [str(int(float(i))) for i in df_geographic_unique['countyCode']]\n",
    "df_geographic_unique['countyCode'] = [censusBG.zfill(5) for censusBG in df_geographic_unique['countyCode']]\n",
    "\n",
    "df_geographic_unique['censusTract'] = [str(int(float(i))) for i in df_geographic_unique['censusTract']]\n",
    "df_geographic_unique['censusTract'] = [censusBG.zfill(11) for censusBG in df_geographic_unique['censusTract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34eadf-775c-41ab-86a4-65fce4f847ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for yearOfLoss_1990_2021\n",
    "\n",
    "bins_1980_2021 = [df['yearOfLoss'].min(), 1980, 1990, 2000, 2010, 2020, df['yearOfLoss'].max() + 1]\n",
    "labels_1980_2021 = [0, 1980, 1990, 2000, 2010, 2020]\n",
    "\n",
    "df_geographic_unique['yearOfLoss_1990_2021'] = pd.cut(df_geographic_unique['yearOfLoss'], bins=bins_1990_2021, labels=labels_1990_2021, right=False).astype(int)\n",
    "\n",
    "\n",
    "df_geographic_unique = df_geographic_unique.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15378d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adjusting the bins so that 2011 falls into the 2012 bin\n",
    "custom_bins = [0, 2000] + list(range(2010, 2023)) + [2024]\n",
    "\n",
    "# Step 2: Define the labels.\n",
    "# Adjusting the labels to reflect the new bin structure\n",
    "custom_labels = [0, 2000] + list(range(2010, 2023))  # The label 2022 will apply to both 2022 and 2023.\n",
    "\n",
    "# Ensure the number of labels is one less than the number of bin edges.\n",
    "assert len(custom_bins) == len(custom_labels) + 1, \"Number of bin labels must be one fewer than the number of bin edges.\"\n",
    "\n",
    "# Step 3: Apply the custom binning and create the 'zip_year_bin' column.\n",
    "df_geographic_unique['zip_year_bin'] = pd.cut(\n",
    "    df_geographic_unique['yearOfLoss'],\n",
    "    bins=custom_bins,\n",
    "    labels=custom_labels,\n",
    "    right=False,  # Ensuring the rightmost edge is exclusive.\n",
    "    include_lowest=True,  # Including the leftmost edge.\n",
    ").astype(int)  # Converting the resulting categories to integers for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62857dd5-d024-49aa-9a2e-03e131427586",
   "metadata": {},
   "source": [
    "## Read shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d48f5-4ac1-4047-84d8-0d2c2b724253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#State shapefile\n",
    "\n",
    "states = pygris.states()\n",
    "\n",
    "state_df = states[['STUSPS', 'NAME', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412cfb7-312e-4011-876e-842c2d361c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lat-Long shapefile\n",
    "df_read = pd.read_parquet(\"C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/lat_long_geometry.parquet.gzip\")\n",
    "\n",
    "# Convert the WKT strings back to geometries\n",
    "lat_long_df = gpd.GeoDataFrame(df_read, geometry=df_read['geometry'].apply(lambda x: shapely.wkt.loads(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48003008-1a2a-4ad4-a67d-4089a23ea76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Blockgroup shapefile\n",
    "\n",
    "chunk_size = 40000 \n",
    "chunks = [x for x in range(0, 120000, chunk_size)]\n",
    "\n",
    "gdf_list = []\n",
    "\n",
    "for start in chunks:\n",
    "    end = start + chunk_size\n",
    "    temp_df = pd.read_parquet(f\"C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/BG_geometry_{start}_{end}.parquet.gzip\")\n",
    "    gdf_read = gpd.GeoDataFrame(temp_df, geometry=temp_df['geometry'].apply(lambda x: shapely.wkt.loads(x)))\n",
    "    gdf_list.append(gdf_read)\n",
    "\n",
    "# Concatenate all GeoDataFrames in the list into a single GeoDataFrame\n",
    "BG_df= pd.concat(gdf_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b26459-fdea-4fc4-a70c-f9d92ffa3412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 50000  # adjust based on your system's capabilities\n",
    "chunks = [x for x in range(0, 300000, chunk_size)]\n",
    "\n",
    "gdf_list = []\n",
    "\n",
    "for start in chunks:\n",
    "    end = start + chunk_size\n",
    "    temp_df = pd.read_parquet(f\"C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/zipcode_geometry_{start}_{end}.parquet.gzip\")\n",
    "    gdf_read = gpd.GeoDataFrame(temp_df, geometry=temp_df['geometry'].apply(lambda x: shapely.wkt.loads(x)))\n",
    "    gdf_list.append(gdf_read)\n",
    "    \n",
    "# Concatenate all GeoDataFrames in the list into a single GeoDataFrame\n",
    "zipcode_df = pd.concat(gdf_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ae2c2-b10a-474e-9010-fe9b88389e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read shapefile of county code\n",
    "df_read = pd.read_parquet(\"C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/County_geometry.parquet.gzip\")\n",
    "\n",
    "# Convert the WKT strings back to geometries\n",
    "County_df = gpd.GeoDataFrame(df_read, geometry=df_read['geometry'].apply(lambda x: shapely.wkt.loads(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e967f03-fba1-4e28-be9b-ff0c7d90112e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read shapefile of census tract code\n",
    "chunk_size = 30000 \n",
    "chunks = [x for x in range(0, 60000, chunk_size)]\n",
    "\n",
    "gdf_list = []\n",
    "\n",
    "for start in chunks:\n",
    "    end = start + chunk_size\n",
    "    temp_df = pd.read_parquet(f\"C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/Tract_geometry_{start}_{end}.parquet.gzip\")\n",
    "    gdf_read = gpd.GeoDataFrame(temp_df, geometry=temp_df['geometry'].apply(lambda x: shapely.wkt.loads(x)))\n",
    "    gdf_list.append(gdf_read)\n",
    "\n",
    "# Concatenate all GeoDataFrames in the list into a single GeoDataFrame\n",
    "Tract_df= pd.concat(gdf_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11888fc-3815-4548-92de-3962f23d46be",
   "metadata": {},
   "source": [
    "### Mapping Geometries to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05777118-f02a-4c1c-a993-e930cae14135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_df.rename(columns={'geometry': 'geometry_state'}, inplace=True)\n",
    "lat_long_df.rename(columns={'geometry': 'geometry_lat_long'}, inplace=True)\n",
    "BG_df.rename(columns={'geometry': 'geometry_BG'}, inplace=True)\n",
    "zipcode_df.rename(columns={'geometry': 'geometry_zipcode'}, inplace=True)\n",
    "County_df.rename(columns={'geometry': 'geometry_county'}, inplace=True)\n",
    "Tract_df.rename(columns={'geometry': 'geometry_tract'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449750a4-63eb-4a8e-aa62-08aac8405021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the multi-index on lat_long_df\n",
    "lat_long_df.set_index(['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Mapping the values\n",
    "df_geographic_unique['geometry_lat_long'] = df_geographic_unique.set_index(['latitude', 'longitude']).index.map(lat_long_df['geometry_lat_long'])\n",
    "\n",
    "lat_long_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79b534-3d24-4218-a9bb-7ab66abca39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "BG_df.set_index(['GEOID'], inplace=True)\n",
    "df_geographic_unique['geometry_BG'] = df_geographic_unique.set_index(['censusBlockGroupFips']).index.map(BG_df['geometry_BG'])\n",
    "\n",
    "BG_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abba0e4-994c-4b0f-8636-c251309d9fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "zipcode_df.set_index(['ZIPcode', 'year'], inplace=True)\n",
    "df_geographic_unique['geometry_zipcode'] = df_geographic_unique.set_index(['reportedZipCode', 'zip_year_bin']).index.map(zipcode_df['geometry_zipcode'])\n",
    "\n",
    "zipcode_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b83781-66d7-4642-9eaf-c0887dde5302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "state_df.set_index(['STUSPS'], inplace=True)\n",
    "\n",
    "# Mapping the values\n",
    "df_geographic_unique['geometry_state'] = df_geographic_unique.set_index(['state']).index.map(state_df['geometry_state'])\n",
    "\n",
    "state_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d0ec8-8b97-449f-a184-47633485fe98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "County_df.set_index(['CountyID'], inplace=True)\n",
    "df_geographic_unique['geometry_county'] = df_geographic_unique.set_index(['countyCode']).index.map(County_df['geometry_county'])\n",
    "\n",
    "# Resetting the index of County_df (return to multi-index)\n",
    "County_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85003161-f262-4ceb-bbd2-a76731e85cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "Tract_df.set_index(['censusTractID'], inplace=True)\n",
    "df_geographic_unique['geometry_tract'] = df_geographic_unique.set_index(['censusTract']).index.map(Tract_df['geometry_tract'])\n",
    "\n",
    "# Resetting the index of County_df (return to multi-index)\n",
    "Tract_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019edc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a GeoDataFrame named df_geographic_unique\n",
    "# Make a copy of the original GeoDataFrame to avoid modifying it\n",
    "df_copy = df_geographic_unique.copy()\n",
    "\n",
    "# Extract the original geometry of the first observation at index 1\n",
    "original_geometry = df_copy['geometry_zipcode'].iloc[1]\n",
    "\n",
    "# Calculate the original area\n",
    "original_area = original_geometry.area\n",
    "\n",
    "# Define a target area which is 10% greater than the original area\n",
    "target_area = original_area * 1.10\n",
    "\n",
    "# Initialize the buffer distance and the buffered area\n",
    "buffer_distance = 0\n",
    "buffered_area = 0\n",
    "\n",
    "# Incrementally increase the buffer distance until the area of the buffered shape\n",
    "# is at least as large as the target area\n",
    "while buffered_area < target_area:\n",
    "    buffer_distance += 0.00001  # Increment buffer_distance by a small amount\n",
    "    buffered_geometry = original_geometry.buffer(buffer_distance)\n",
    "    buffered_area = buffered_geometry.area\n",
    "\n",
    "# Create a GeoDataFrame with just the buffered geometry\n",
    "gdf_buffered = gpd.GeoDataFrame({'geometry': [buffered_geometry]}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame with just the original geometry\n",
    "gdf_original = gpd.GeoDataFrame({'geometry': [original_geometry]}, crs=\"EPSG:4326\")\n",
    "\n",
    "# Create subplots for before, after, and combined plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # Adjusted for 3 subplots\n",
    "\n",
    "# Plot the original geometry at index 1 before the buffer using GeoPandas\n",
    "gdf_original.plot(ax=axes[0], color='blue', alpha=0.5)\n",
    "axes[0].set_title('Original Geometry (EPSG:4326)')\n",
    "\n",
    "# Plot only the buffered geometry\n",
    "gdf_buffered.plot(ax=axes[1], color='red', alpha=0.5)\n",
    "axes[1].set_title('Buffered Geometry (EPSG:4326)')\n",
    "\n",
    "# Plot the buffered geometry first, then the original geometry on the same subplot for comparison\n",
    "gdf_buffered.plot(ax=axes[2], color='red', alpha=0.3)  # Use a lighter alpha for the buffer\n",
    "gdf_original.plot(ax=axes[2], color='blue', alpha=0.5)\n",
    "axes[2].set_title('Comparison of Original and Buffered Geometries (EPSG:4326)')\n",
    "\n",
    "# Highlight the buffer by plotting the difference between the buffered and original geometries\n",
    "buffer_difference = gdf_buffered['geometry'].difference(gdf_original['geometry'].iloc[0])\n",
    "gpd.GeoDataFrame({'geometry': buffer_difference}, crs=\"EPSG:4326\").plot(ax=axes[2], color='yellow', alpha=0.5)\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "# Print the areas before and after the buffer\n",
    "print(f\"Original Area: {original_area}\")\n",
    "print(f\"Buffered Area: {buffered_area}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866def6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb9254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e422c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e7b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2a97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5a9465b-e8fb-4fc0-a3b4-dfea0e7c9b18",
   "metadata": {},
   "source": [
    "### Adding the inconsistent flags to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13d0b8-c14d-4ef0-93e7-78521d7fe691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1_inconsistent = pd.read_parquet('C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/InconsistencyDataframes/inconsistency_dataframe_1.parquet.gzip')\n",
    "df_2_inconsistent = pd.read_parquet('C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/InconsistencyDataframes/inconsistency_dataframe_2.parquet.gzip')\n",
    "df_3_inconsistent = pd.read_parquet('C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/InconsistencyDataframes/inconsistency_dataframe_3.parquet.gzip')\n",
    "df_4_inconsistent = pd.read_parquet('C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/InconsistencyDataframes/inconsistency_dataframe_4.parquet.gzip')\n",
    "df_5_inconsistent = pd.read_parquet('C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/InconsistencyDataframes/inconsistency_dataframe_5.parquet.gzip')\n",
    "df_6_inconsistent = pd.read_parquet('C:/Users/Jorda/Box/Flood Damage PredictionProject/Dataset/InconsistencyDataframes/inconsistency_dataframe_6.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d5fc9-1747-46ff-acd9-44b5fc538e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(sum((df_1_inconsistent['zipInconsistent'] == 1.0) & (df_1_inconsistent['oneWrong'] == 1.0)) + \n",
    " sum((df_3_inconsistent['zipInconsistent'] == 1.0) & (df_3_inconsistent['oneWrong'] == 1.0)) +\n",
    " sum((df_5_inconsistent['zipInconsistent'] == 1.0) & (df_5_inconsistent['oneWrong'] == 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd167e-d72f-416d-9aee-b909dbd2a5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(sum((df_1_inconsistent['stateInconsistent'] == 1.0) & (df_1_inconsistent['oneWrong'] == 1.0)) + \n",
    "  sum((df_2_inconsistent['stateInconsistent'] == 1.0) & (df_2_inconsistent['oneWrong'] == 1.0)) +\n",
    " sum((df_3_inconsistent['stateInconsistent'] == 1.0) & (df_3_inconsistent['oneWrong'] == 1.0)) +\n",
    "  sum((df_4_inconsistent['stateInconsistent'] == 1.0) & (df_4_inconsistent['oneWrong'] == 1.0)) +\n",
    " sum((df_5_inconsistent['stateInconsistent'] == 1.0) & (df_5_inconsistent['oneWrong'] == 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6bce15-0524-4cf9-993f-1925d30d7c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_2_inconsistent\n",
    "df_2_inconsistent['reportedZipCode'] = df_2_inconsistent['reportedZipCode'].fillna(0)\n",
    "\n",
    "#df_3_inconsistent\n",
    "df_3_inconsistent['countyCode'] = df_3_inconsistent['countyCode'].fillna(0)\n",
    "\n",
    "#df_4_inconsistent\n",
    "df_4_inconsistent['reportedZipCode'] = df_4_inconsistent['reportedZipCode'].fillna(0)\n",
    "df_4_inconsistent['countyCode'] = df_4_inconsistent['countyCode'].fillna(0)\n",
    "\n",
    "#df_5_inconsistent\n",
    "df_5_inconsistent['censusTract'] = df_5_inconsistent['censusTract'].fillna(0)\n",
    "df_5_inconsistent['censusBlockGroupFips'] = df_5_inconsistent['censusBlockGroupFips'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d20eed-1866-4816-b00b-683722a2020c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert the units to string to use mapping and fix the lenght of each units\n",
    "#df_2_inconsistent\n",
    "df_2_inconsistent['reportedZipCode'] = df_2_inconsistent['reportedZipCode'].astype(int).astype(str)\n",
    "df_2_inconsistent['reportedZipCode'] = [zipcode.zfill(5) for zipcode in df_2_inconsistent['reportedZipCode']]\n",
    "\n",
    "#df_3_inconsistent\n",
    "df_3_inconsistent['countyCode'] = [str(int(float(i))) for i in df_3_inconsistent['countyCode']]\n",
    "df_3_inconsistent['countyCode'] = [censusBG.zfill(5) for censusBG in df_3_inconsistent['countyCode']]\n",
    "\n",
    "#df_4_inconsistent\n",
    "df_4_inconsistent['reportedZipCode'] = df_4_inconsistent['reportedZipCode'].astype(int).astype(str)\n",
    "df_4_inconsistent['reportedZipCode'] = [zipcode.zfill(5) for zipcode in df_4_inconsistent['reportedZipCode']]\n",
    "\n",
    "df_4_inconsistent['countyCode'] = [str(int(float(i))) for i in df_4_inconsistent['countyCode']]\n",
    "df_4_inconsistent['countyCode'] = [censusBG.zfill(5) for censusBG in df_4_inconsistent['countyCode']]\n",
    "\n",
    "#df_5_inconsistent\n",
    "df_5_inconsistent['censusBlockGroupFips'] = [str(int(float(i))) for i in df_5_inconsistent['censusBlockGroupFips']]\n",
    "df_5_inconsistent['censusBlockGroupFips'] = [censusBG.zfill(12) for censusBG in df_5_inconsistent['censusBlockGroupFips']]\n",
    "\n",
    "df_5_inconsistent['censusTract'] = [str(int(float(i))) for i in df_5_inconsistent['censusTract']]\n",
    "df_5_inconsistent['censusTract'] = [censusBG.zfill(11) for censusBG in df_5_inconsistent['censusTract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a43352-6ab0-4f8d-a920-442e67f33897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1_inconsistent.drop(columns=['zip_geometry','lat_long_geometry','bg_geometry'], inplace=True)\n",
    "df_2_inconsistent.drop(columns=['lat_long_geometry','state_geometry','county_geometry'], inplace=True)\n",
    "df_3_inconsistent.drop(columns=['state_geometry','zip_geometry'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef6b44-5310-4761-89af-e88c643d0f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_geographic_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296c0c8-2d9f-46d1-9093-30f53f1b2ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1_inconsistent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_1_inconsistent.index.is_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that were intended to be set as index\n",
    "index_columns = ['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                 'latitude', 'longitude', 'year']\n",
    "\n",
    "# Ensure the DataFrame index is reset to default\n",
    "df_reset = df_1_inconsistent.reset_index()\n",
    "\n",
    "# Find duplicate rows based on the intended index columns\n",
    "duplicates_df = df_reset[df_reset.duplicated(subset=index_columns, keep=False)]\n",
    "\n",
    "# If you want to display only the first few instances of duplicates, you can use `.head()`\n",
    "print(duplicates_df.head())  # Adjust the number inside head() to control the number of rows displayed\n",
    "\n",
    "# If there are many duplicates and you want to see all, you would simply print the variable:\n",
    "# print(duplicates_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c955c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b280cec-d7d5-45ce-a52d-0e87679b898e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_1_inconsistent.drop_duplicates(subset=['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude','year', 'year_zipcode'], inplace=True)\n",
    "\n",
    "# Initial mapping with multi-index\n",
    "df_1_inconsistent.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'year', 'year_zipcode'], inplace=True)\n",
    "\n",
    "df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips', \n",
    "                                'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin'], inplace=True)\n",
    "\n",
    "df_geographic_unique['zipInconsistent'] = df_geographic_unique.index.map(df_1_inconsistent['zipInconsistent'])\n",
    "\n",
    "# Reset the index\n",
    "df_geographic_unique.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "df_geographic_unique['zipInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['zipInconsistent'])\n",
    "df_geographic_unique['cbgInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['cbgInconsistent'])\n",
    "df_geographic_unique['tractInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['tractInconsistent'])\n",
    "df_geographic_unique['stateInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['stateInconsistent'])\n",
    "df_geographic_unique['countyInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['countyInconsistent'])\n",
    "df_geographic_unique['latlongInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['latlongInconsistent'])\n",
    "df_geographic_unique['multiple_inconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['multiple'])\n",
    "df_geographic_unique['single_inconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index.map(df_1_inconsistent['oneWrong'])\n",
    "\n",
    "# Resetting the index (return to multi-index)\n",
    "df_1_inconsistent.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d27dd9-a62a-4f97-8e6f-0710e358a53a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2_inconsistent.drop_duplicates(subset=['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude','year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4fce4-e1f5-4711-9e7f-630d1325ccac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "df_2_inconsistent.set_index(['reportedZipCode','censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'year'], inplace=True)\n",
    "\n",
    "multi_index = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021']).index\n",
    "cbgInconsistent = pd.Series(multi_index.map(df_2_inconsistent['cbgInconsistent']), index=multi_index)\n",
    "tractInconsistent = pd.Series(multi_index.map(df_2_inconsistent['tractInconsistent']), index=multi_index)\n",
    "stateInconsistent = pd.Series(multi_index.map(df_2_inconsistent['stateInconsistent']), index=multi_index)\n",
    "countyInconsistent = pd.Series(multi_index.map(df_2_inconsistent['countyInconsistent']), index=multi_index)\n",
    "latlongInconsistent = pd.Series(multi_index.map(df_2_inconsistent['latlongInconsistent']), index=multi_index)\n",
    "multiple_inconsistent = pd.Series(multi_index.map(df_2_inconsistent['multiple']), index=multi_index)\n",
    "single_inconsistent = pd.Series(multi_index.map(df_2_inconsistent['oneWrong']), index=multi_index)\n",
    "\n",
    "# Revert df_2_inconsistent back to its original form\n",
    "df_2_inconsistent.reset_index(inplace=True)\n",
    "\n",
    "# Fill NaN values in df_geographic_unique['cbgInconsistent'] with values from cbgInconsistent Series\n",
    "df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips', 'latitude', \n",
    "                                'longitude', 'yearOfLoss_1990_2021'], inplace=True)\n",
    "df_geographic_unique['cbgInconsistent'].fillna(cbgInconsistent, inplace=True)\n",
    "df_geographic_unique['tractInconsistent'].fillna(tractInconsistent, inplace=True)\n",
    "df_geographic_unique['stateInconsistent'].fillna(stateInconsistent, inplace=True)\n",
    "df_geographic_unique['countyInconsistent'].fillna(countyInconsistent, inplace=True)\n",
    "df_geographic_unique['latlongInconsistent'].fillna(latlongInconsistent, inplace=True)\n",
    "df_geographic_unique['multiple_inconsistent'].fillna(multiple_inconsistent, inplace=True)\n",
    "df_geographic_unique['single_inconsistent'].fillna(single_inconsistent, inplace=True)\n",
    "\n",
    "# Reset index for df_geographic_unique if needed\n",
    "df_geographic_unique.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2782ff-9f78-43b9-8e9d-5a4978592be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initial mapping with multi-index\n",
    "# df_2_inconsistent.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# df_geographic_unique['cbgInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['cbgInconsistent'])\n",
    "# df_geographic_unique['tractInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['tractInconsistent'])\n",
    "# df_geographic_unique['stateInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['stateInconsistent'])\n",
    "# df_geographic_unique['countyInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['countyInconsistent'])\n",
    "# df_geographic_unique['latlongInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['latlongInconsistent'])\n",
    "# df_geographic_unique['multiple_inconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['multiple'])\n",
    "# df_geographic_unique['single_inconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['oneWrong'])\n",
    "\n",
    "# # Resetting the index of County_df (return to multi-index)\n",
    "# df_2_inconsistent.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c290d2-11b8-4e60-8650-9b6ddce2a9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_3_inconsistent.drop_duplicates(subset=['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude','year', 'year_zipcode'], inplace=True)\n",
    "\n",
    "# Initial mapping with multi-index\n",
    "df_3_inconsistent.set_index(['reportedZipCode','censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'year', 'year_zipcode'], inplace=True)\n",
    "\n",
    "multi_index = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index\n",
    "zipInconsistent = pd.Series(multi_index.map(df_3_inconsistent['zipInconsistent']), index=multi_index)\n",
    "cbgInconsistent = pd.Series(multi_index.map(df_3_inconsistent['cbgInconsistent']), index=multi_index)\n",
    "tractInconsistent = pd.Series(multi_index.map(df_3_inconsistent['tractInconsistent']), index=multi_index)\n",
    "stateInconsistent = pd.Series(multi_index.map(df_3_inconsistent['stateInconsistent']), index=multi_index)\n",
    "# countyInconsistent = pd.Series(multi_index.map(df_3_inconsistent['countyInconsistent']), index=multi_index)\n",
    "latlongInconsistent = pd.Series(multi_index.map(df_3_inconsistent['latlongInconsistent']), index=multi_index)\n",
    "multiple_inconsistent = pd.Series(multi_index.map(df_3_inconsistent['multiple']), index=multi_index)\n",
    "single_inconsistent = pd.Series(multi_index.map(df_3_inconsistent['oneWrong']), index=multi_index)\n",
    "\n",
    "# Revert df_3_inconsistent back to its original form\n",
    "df_3_inconsistent.reset_index(inplace=True)\n",
    "\n",
    "# Fill NaN values in df_geographic_unique['cbgInconsistent'] with values from cbgInconsistent Series\n",
    "df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips', 'latitude', \n",
    "                                'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin'], inplace=True)\n",
    "df_geographic_unique['zipInconsistent'].fillna(zipInconsistent, inplace=True)\n",
    "df_geographic_unique['cbgInconsistent'].fillna(cbgInconsistent, inplace=True)\n",
    "df_geographic_unique['tractInconsistent'].fillna(tractInconsistent, inplace=True)\n",
    "df_geographic_unique['stateInconsistent'].fillna(stateInconsistent, inplace=True)\n",
    "# df_geographic_unique['countyInconsistent'].fillna(countyInconsistent, inplace=True)\n",
    "df_geographic_unique['latlongInconsistent'].fillna(latlongInconsistent, inplace=True)\n",
    "df_geographic_unique['multiple_inconsistent'].fillna(multiple_inconsistent, inplace=True)\n",
    "df_geographic_unique['single_inconsistent'].fillna(single_inconsistent, inplace=True)\n",
    "\n",
    "# Reset index for df_geographic_unique if needed\n",
    "df_geographic_unique.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fce1c3-3b1c-4f28-9b7e-396324d83b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initial mapping with multi-index\n",
    "# df_3_inconsistent.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'year'], inplace=True)\n",
    "\n",
    "# df_geographic_unique['zipInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_3_inconsistent['zipInconsistent'])\n",
    "# df_geographic_unique['cbgInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_3_inconsistent['cbgInconsistent'])\n",
    "# df_geographic_unique['tractInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_3_inconsistent['tractInconsistent'])\n",
    "# df_geographic_unique['stateInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_3_inconsistent['stateInconsistent'])\n",
    "# df_geographic_unique['latlongInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_3_inconsistent['latlongInconsistent'])\n",
    "# df_geographic_unique['multiple_inconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_3_inconsistent['multiple'])\n",
    "# df_geographic_unique['single_inconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_3_inconsistent['oneWrong'])\n",
    "\n",
    "# # Resetting the index of County_df (return to multi-index)\n",
    "# df_3_inconsistent.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b07384-508a-4806-b093-7be560159a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_4_inconsistent.drop_duplicates(subset=['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude','year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaacc54-98e9-4c67-88a8-023fb66372a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "df_4_inconsistent.set_index(['reportedZipCode','censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'year'], inplace=True)\n",
    "\n",
    "multi_index = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021']).index\n",
    "# zipInconsistent = pd.Series(multi_index.map(df_4_inconsistent['zipInconsistent']), index=multi_index)\n",
    "cbgInconsistent = pd.Series(multi_index.map(df_4_inconsistent['cbgInconsistent']), index=multi_index)\n",
    "tractInconsistent = pd.Series(multi_index.map(df_4_inconsistent['tractInconsistent']), index=multi_index)\n",
    "stateInconsistent = pd.Series(multi_index.map(df_4_inconsistent['stateInconsistent']), index=multi_index)\n",
    "# countyInconsistent = pd.Series(multi_index.map(df_3_inconsistent['countyInconsistent']), index=multi_index)\n",
    "latlongInconsistent = pd.Series(multi_index.map(df_4_inconsistent['latlongInconsistent']), index=multi_index)\n",
    "multiple_inconsistent = pd.Series(multi_index.map(df_4_inconsistent['multiple']), index=multi_index)\n",
    "single_inconsistent = pd.Series(multi_index.map(df_4_inconsistent['oneWrong']), index=multi_index)\n",
    "\n",
    "# Revert df_4_inconsistent back to its original form\n",
    "df_4_inconsistent.reset_index(inplace=True)\n",
    "\n",
    "# Fill NaN values in df_geographic_unique['cbgInconsistent'] with values from cbgInconsistent Series\n",
    "df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips', 'latitude', \n",
    "                                'longitude', 'yearOfLoss_1990_2021'], inplace=True)\n",
    "# df_geographic_unique['zipInconsistent'].fillna(zipInconsistent, inplace=True)\n",
    "df_geographic_unique['cbgInconsistent'].fillna(cbgInconsistent, inplace=True)\n",
    "df_geographic_unique['tractInconsistent'].fillna(tractInconsistent, inplace=True)\n",
    "df_geographic_unique['stateInconsistent'].fillna(stateInconsistent, inplace=True)\n",
    "# df_geographic_unique['countyInconsistent'].fillna(countyInconsistent, inplace=True)\n",
    "df_geographic_unique['latlongInconsistent'].fillna(latlongInconsistent, inplace=True)\n",
    "df_geographic_unique['multiple_inconsistent'].fillna(multiple_inconsistent, inplace=True)\n",
    "df_geographic_unique['single_inconsistent'].fillna(single_inconsistent, inplace=True)\n",
    "\n",
    "# Reset index for df_geographic_unique if needed\n",
    "df_geographic_unique.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93975c-b59a-4285-ac6c-e990ef903867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initial mapping with multi-index\n",
    "# df_4_inconsistent.set_index(['censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# df_geographic_unique['cbgInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_4_inconsistent['cbgInconsistent'])\n",
    "# df_geographic_unique['tractInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_4_inconsistent['tractInconsistent'])\n",
    "# df_geographic_unique['stateInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_4_inconsistent['stateInconsistent'])\n",
    "# df_geographic_unique['latlongInconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_4_inconsistent['latlongInconsistent'])\n",
    "# df_geographic_unique['multiple_inconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_4_inconsistent['multiple'])\n",
    "# df_geographic_unique['single_inconsistent'] = df_geographic_unique.set_index(['censusTract', 'state', 'censusBlockGroupFips',\n",
    "#                             'latitude', 'longitude']).index.map(df_4_inconsistent['oneWrong'])\n",
    "\n",
    "# # Resetting the index of County_df (return to multi-index)\n",
    "# df_4_inconsistent.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede0c1a-2834-47af-aa87-0100881fce4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_5_inconsistent.drop_duplicates(subset=['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude','year','year_zipcode'], inplace=True)\n",
    "\n",
    "# Initial mapping with multi-index\n",
    "df_5_inconsistent.set_index(['reportedZipCode','censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'year', 'year_zipcode'], inplace=True)\n",
    "\n",
    "multi_index = df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips',\n",
    "                            'latitude', 'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin']).index\n",
    "zipInconsistent = pd.Series(multi_index.map(df_5_inconsistent['zipInconsistent']), index=multi_index)\n",
    "# cbgInconsistent = pd.Series(multi_index.map(df_5_inconsistent['cbgInconsistent']), index=multi_index)\n",
    "# tractInconsistent = pd.Series(multi_index.map(df_5_inconsistent['tractInconsistent']), index=multi_index)\n",
    "stateInconsistent = pd.Series(multi_index.map(df_5_inconsistent['stateInconsistent']), index=multi_index)\n",
    "countyInconsistent = pd.Series(multi_index.map(df_5_inconsistent['countyInconsistent']), index=multi_index)\n",
    "latlongInconsistent = pd.Series(multi_index.map(df_5_inconsistent['latlongInconsistent']), index=multi_index)\n",
    "multiple_inconsistent = pd.Series(multi_index.map(df_5_inconsistent['multiple']), index=multi_index)\n",
    "single_inconsistent = pd.Series(multi_index.map(df_5_inconsistent['oneWrong']), index=multi_index)\n",
    "\n",
    "# Revert df_5_inconsistent back to its original form\n",
    "df_5_inconsistent.reset_index(inplace=True)\n",
    "\n",
    "# Fill NaN values in df_geographic_unique['cbgInconsistent'] with values from cbgInconsistent Series\n",
    "df_geographic_unique.set_index(['reportedZipCode', 'censusTract', 'state', 'countyCode', 'censusBlockGroupFips', 'latitude', \n",
    "                                'longitude', 'yearOfLoss_1990_2021', 'zip_year_bin'], inplace=True)\n",
    "df_geographic_unique['zipInconsistent'].fillna(zipInconsistent, inplace=True)\n",
    "# df_geographic_unique['cbgInconsistent'].fillna(cbgInconsistent, inplace=True)\n",
    "# df_geographic_unique['tractInconsistent'].fillna(tractInconsistent, inplace=True)\n",
    "df_geographic_unique['stateInconsistent'].fillna(stateInconsistent, inplace=True)\n",
    "df_geographic_unique['countyInconsistent'].fillna(countyInconsistent, inplace=True)\n",
    "df_geographic_unique['latlongInconsistent'].fillna(latlongInconsistent, inplace=True)\n",
    "df_geographic_unique['multiple_inconsistent'].fillna(multiple_inconsistent, inplace=True)\n",
    "df_geographic_unique['single_inconsistent'].fillna(single_inconsistent, inplace=True)\n",
    "\n",
    "# Reset index for df_geographic_unique if needed\n",
    "df_geographic_unique.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc3292-8fdb-45e0-9a20-4931f327e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initial mapping with multi-index\n",
    "# df_5_inconsistent.set_index(['reportedZipCode', 'state', 'countyCode',\n",
    "#                             'latitude', 'longitude', 'year'], inplace=True)\n",
    "\n",
    "# df_geographic_unique['zipInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'state', 'countyCode',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_5_inconsistent['zipInconsistent'])\n",
    "# df_geographic_unique['stateInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'state', 'countyCode',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_5_inconsistent['stateInconsistent'])\n",
    "# df_geographic_unique['countyInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'state', 'countyCode',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_5_inconsistent['countyInconsistent'])\n",
    "# df_geographic_unique['latlongInconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'state', 'countyCode',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_5_inconsistent['latlongInconsistent'])\n",
    "# df_geographic_unique['multiple_inconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'state', 'countyCode',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_5_inconsistent['multiple'])\n",
    "# df_geographic_unique['single_inconsistent'] = df_geographic_unique.set_index(['reportedZipCode', 'state', 'countyCode',\n",
    "#                             'latitude', 'longitude', 'yearOfLoss_1990_2021']).index.map(df_5_inconsistent['oneWrong'])\n",
    "\n",
    "# # Resetting the index of County_df (return to multi-index)\n",
    "# df_5_inconsistent.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f9996-c421-433e-9533-9cb04dbabd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initial mapping with multi-index (df_6_inconsistent is empty)\n",
    "# df_6_inconsistent.set_index(['state', 'countyCode',\n",
    "#                             'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# df_geographic_unique['stateInconsistent'] = df_geographic_unique.set_index(['state', 'countyCode',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['stateInconsistent'])\n",
    "# df_geographic_unique['countyInconsistent'] = df_geographic_unique.set_index(['state', 'countyCode',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['countyInconsistent'])\n",
    "# df_geographic_unique['latlongInconsistent'] = df_geographic_unique.set_index(['state', 'countyCode',\n",
    "#                             'latitude', 'longitude']).index.map(df_2_inconsistent['latlongInconsistent'])\n",
    "\n",
    "# # Resetting the index of County_df (return to multi-index)\n",
    "# df_6_inconsistent.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb255793-3672-4c31-b32c-a2d38d49f8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_geographic_unique[df_geographic_unique['zipInconsistent'].notna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9eb15e-b893-4051-9715-09789ed61a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = df_geographic_unique[df_geographic_unique['single_inconsistent'] == 1.0]\n",
    "\n",
    "print(f\"zipInconsistent: {sum(test['zipInconsistent'] == 1.0)}\")\n",
    "print(f\"cbgInconsistent: {sum(test['cbgInconsistent'] == 1.0)}\")\n",
    "print(f\"tractInconsistent: {sum((test['tractInconsistent'] == 1.0))}\")\n",
    "print(f\"stateInconsistent: {sum((test['stateInconsistent'] == 1.0))}\")\n",
    "print(f\"countyInconsistent: {sum((test['countyInconsistent'] == 1.0))}\")\n",
    "print(f\"latlongInconsistent: {sum((test['latlongInconsistent'] == 1.0))}\")\n",
    "print(f\"multiple_inconsistent: {sum((test['multiple_inconsistent'] == 1.0))}\")\n",
    "print(f\"single_inconsistent: {sum((test['single_inconsistent'] == 1.0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90faec-83fa-47fe-9a66-3e3d3a59de2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"zipInconsistent: {sum((df_geographic_unique['zipInconsistent'].notna()) & (df_geographic_unique['zipInconsistent'] == 1.0))}\")\n",
    "print(f\"cbgInconsistent: {sum((df_geographic_unique['cbgInconsistent'].notna()) & (df_geographic_unique['cbgInconsistent'] == 1.0))}\")\n",
    "print(f\"tractInconsistent: {sum((df_geographic_unique['tractInconsistent'].notna()) & (df_geographic_unique['tractInconsistent'] == 1.0))}\")\n",
    "print(f\"stateInconsistent: {sum((df_geographic_unique['stateInconsistent'].notna()) & (df_geographic_unique['stateInconsistent'] == 1.0))}\")\n",
    "print(f\"countyInconsistent: {sum((df_geographic_unique['countyInconsistent'].notna()) & (df_geographic_unique['countyInconsistent'] == 1.0))}\")\n",
    "print(f\"latlongInconsistent: {sum((df_geographic_unique['latlongInconsistent'].notna()) & (df_geographic_unique['latlongInconsistent'] == 1.0))}\")\n",
    "print(f\"multiple_inconsistent: {sum((df_geographic_unique['multiple_inconsistent'].notna()) & (df_geographic_unique['multiple_inconsistent'] == 1.0))}\")\n",
    "print(f\"single_inconsistent: {sum((df_geographic_unique['single_inconsistent'].notna()) & (df_geographic_unique['single_inconsistent'] == 1.0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea2f25-2f9c-40f3-b46d-83ebac0f5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: for all NA values change it to 1\n",
    "#TODO: create 'multipleInconsistent' column where add all flags\n",
    "#TODO: for state = 'UN', make it N\n",
    "#TODO (Not here but later): for missing geometries, change flag to M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66557d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geographic_unique['zipInconsistent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ca749",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geographic_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_none =(df_geographic_unique['geometry_lat_long'] == None).sum()\n",
    "print(count_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c78977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty_or_none(geometry):\n",
    "    return geometry is None or geometry.is_empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty_or_none_or_no_area(geometry):\n",
    "    return (geometry is None) or (geometry.is_empty) or (geometry.area == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_counts = {\n",
    "    \"bg_zip_not_empty\": 0,\n",
    "    \"bg_no_zip_before_2000\": 0,\n",
    "    \"no_bg_zip_after_2000\": 0,\n",
    "    \"no_bg_no_zip\": 0,\n",
    "    \"lat_long_none\": 0,\n",
    "    \"lat_long_bg_no_zip_before_2000_1\": 0,\n",
    "    \"lat_long_bg_no_zip_before_2000_2\": 0,\n",
    "    \"lat_long_bg_zip_after_2000\": 0,\n",
    "    \"lat_long_no_bg_zip_after_2000\":0,\n",
    "    \"final_lat_long\": 0,\n",
    "    \"lat_long_zip_after_2000\" : 0,\n",
    "    \"lat_long_only_1\" :0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c246a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geographic_unique[df_geographic_unique['year'] < 1981]['geometry_zipcode'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6faf34-7a79-4016-bdcc-189dcaea91c7",
   "metadata": {},
   "source": [
    "### Creating new geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175725fb-287b-4ced-bf91-e331f3d7d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty GeoDataFrame to store the intersection results\n",
    "new_unit_df = gpd.GeoDataFrame(columns=['reportedZipCode', 'countyCode', 'censusTract','censusBlockGroupFips', 'latitude', \n",
    "                                        'longitude', 'state', 'new_geographic_unit', 'year', 'zip_year_bin', 'geometry_zipcode',\n",
    "                                       'geometry_county', 'geometry_tract','geometry_BG','geometry_lat_long','geometry_state', \n",
    "                                        'geometry_new_unit', 'cbgInconsistent', 'tractInconsistent', 'countyInconsistent', \n",
    "                                        'stateInconsistent', 'latlongInconsistent', 'zipcodeInconsistent','multipleinconsistent','singleinconsistent'])\n",
    "\n",
    "empty = Polygon()\n",
    "\n",
    "# Iterate through each row in BG_df_1990 and each row in lat_long_df to find intersections\n",
    "for idx_unit, row_unit in df_geographic_unique.iterrows():\n",
    "    year = row_unit['yearOfLoss_1990_2021']\n",
    "    year_zipcode = row_unit['zip_year_bin']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    county_geometry = row_unit['geometry_county']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "    cbgInconsistent = row_unit['cbgInconsistent']\n",
    "    tractInconsistent = row_unit['tractInconsistent']\n",
    "    countyInconsistent = row_unit['countyInconsistent']\n",
    "    stateInconsistent = row_unit['stateInconsistent']\n",
    "    latlongInconsistent = row_unit['latlongInconsistent']\n",
    "    zipcodeInconsistent = row_unit['zipInconsistent']\n",
    "    multipleInconsistent = row_unit['multiple_inconsistent']\n",
    "    singleInconsistent = row_unit['single_inconsistent']\n",
    "    new_geographic_unit = '0'\n",
    "    intersection_geometry = empty\n",
    "\n",
    "    if (singleInconsistent == 1) and (latlongInconsistent == 1):\n",
    "\n",
    "        if (bg_geometry is not None) and (zipcode_geometry is not None):\n",
    "            intersection_geometry = bg_geometry.intersection(zipcode_geometry)\n",
    "            assignment_counts[\"bg_zip_not_empty\"] += 1\n",
    "\n",
    "\n",
    "            if intersection_geometry.is_empty:\n",
    "                print(\"OH NO!! number 1\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"bg zip not empty\")\n",
    "                new_geographic_unit = f\"{'0'*4}{'0'*4}{bg_id:12}{zipcode:5}{year_zipcode:4}\"\n",
    "                \n",
    "        elif (bg_geometry is not None) and ((zipcode_geometry is None)):\n",
    "            new_geographic_unit = f\"{'0'*4}{'0'*4}{bg_id:12}{'0'*5}{'0'*4}\"\n",
    "            assignment_counts[\"bg_no_zip_before_2000\"] += 1\n",
    "            intersection_geometry = bg_geometry\n",
    "                \n",
    "        elif (bg_geometry is None) and (zipcode_geometry is not None):\n",
    "            new_geographic_unit = f\"{'0'*4}{'0'*4}{'0'*12}{zipcode:5}{year_zipcode:4}\"\n",
    "            assignment_counts[\"no_bg_zip_after_2000\"] += 1\n",
    "            intersection_geometry = zipcode_geometry\n",
    "\n",
    "        \n",
    "        else:\n",
    "            new_geographic_unit = f\"{'0'*4}{'0'*4}{'0'*12}{'0'*5}{'0'*4}\"\n",
    "            assignment_counts[\"no_bg_no_zip\"] += 1\n",
    "            intersection_geometry = empty\n",
    "                \n",
    "            \n",
    "            \n",
    "    else:     \n",
    "  \n",
    "        # Check for the lat_long_geometry being None\n",
    "        if lat_long_geometry is None:\n",
    "            new_geographic_unit = f\"{'0'*4}{'0'*4}{'0'*12}{'0'*5}{'0'*4}\"\n",
    "            assignment_counts[\"lat_long_none\"] += 1\n",
    "            intersection_geometry = empty\n",
    "    \n",
    "        elif (bg_geometry is None and zipcode_geometry is None):\n",
    "            new_geographic_unit = f\"{'0' if lat >= 0 else '1'}{int(abs(lat) * 10):3}{'0' if long >= 0 else '1'}{int(abs(long) * 10):3}{'0'*12}{'0'*5}{'0'*4}\"\n",
    "            assignment_counts[\"lat_long_only_1\"] += 1\n",
    "            intersection_geometry = lat_long_geometry\n",
    "\n",
    "        elif ((zipcode_geometry is None ) and (not is_empty_or_none(lat_long_geometry.intersection(bg_geometry)))):\n",
    "            new_geographic_unit = f\"{'0' if lat >= 0 else '1'}{int(abs(lat) * 10):3}{'0' if long >= 0 else '1'}{int(abs(long) * 10):3}{bg_id:12}{'0'*5}{'2010':4}\"\n",
    "            assignment_counts[\"lat_long_bg_no_zip_before_2000_1\"] += 1\n",
    "            intersection_geometry = lat_long_geometry.intersection(bg_geometry)\n",
    "\n",
    "        elif (bg_geometry is None and (zipcode_geometry is not None) and (not is_empty_or_none(lat_long_geometry.intersection(zipcode_geometry)))):\n",
    "            new_geographic_unit = f\"{'0' if lat >= 0 else '1'}{int(abs(lat) * 10):3}{'0' if long >= 0 else '1'}{int(abs(long) * 10):3}{'0'*12}{zipcode:5}{year_zipcode:4}\"\n",
    "            assignment_counts[\"lat_long_zip_after_2000\"] += 1\n",
    "            intersection_geometry = lat_long_geometry.intersection(zipcode_geometry)\n",
    "\n",
    "    \n",
    "        elif ((bg_geometry is not None) and (zipcode_geometry is not None)):\n",
    "            if not is_empty_or_none(lat_long_geometry.intersection(bg_geometry).intersection(zipcode_geometry)):\n",
    "                new_geographic_unit = f\"{'0' if lat >= 0 else '1'}{int(abs(lat) * 10):3}{'0' if long >= 0 else '1'}{int(abs(long) * 10):3}{bg_id:12}{zipcode:5}{year_zipcode:4}\"\n",
    "                assignment_counts[\"lat_long_bg_zip_after_2000\"] += 1\n",
    "                intersection_geometry = lat_long_geometry.intersection(bg_geometry).intersection(zipcode_geometry)\n",
    "        \n",
    "            elif not is_empty_or_none(lat_long_geometry.intersection(bg_geometry)):\n",
    "                new_geographic_unit = f\"{'0' if lat >= 0 else '1'}{int(abs(lat) * 10):3}{'0' if long >= 0 else '1'}{int(abs(long) * 10):3}{'0'*12}{'0'*5}{'0'*4}\"\n",
    "                assignment_counts[\"lat_long_bg_no_zip_before_2000_2\"] +=1\n",
    "                intersection_geometry = lat_long_geometry.intersection(bg_geometry)\n",
    "        \n",
    "            elif not is_empty_or_none(lat_long_geometry.intersection(zipcode_geometry)):\n",
    "                new_geographic_unit = f\"{'0' if lat >= 0 else '1'}{int(abs(lat) * 10):3}{'0' if long >= 0 else '1'}{int(abs(long) * 10):3}{'0'*12}{zipcode:5}{year_zipcode:4}\"\n",
    "                assignment_counts['lat_long_no_bg_zip_after_2000'] +=1\n",
    "                intersection_geometry = lat_long_geometry.intersection(zipcode_geometry)\n",
    "        else:\n",
    "                new_geographic_unit = f\"{'0' if lat >= 0 else '1'}{int(abs(lat) * 10):3}{'0' if long >= 0 else '1'}{int(abs(long) * 10):3}{'0'*12}{'0'*5}{'0'*4}\"\n",
    "                assignment_counts['final_lat_long'] +=1\n",
    "                intersection_geometry = lat_long_geometry\n",
    "\n",
    "    \n",
    "    new_unit_df = pd.concat([new_unit_df, pd.DataFrame({\n",
    "        'reportedZipCode': [zipcode],\n",
    "        'countyCode': [county_id],\n",
    "        'censusTract': [tract_id],\n",
    "        'censusBlockGroupFips': [bg_id],\n",
    "        'latitude': [lat],\n",
    "        'longitude': [long],\n",
    "        'state': [state],\n",
    "        'new_geographic_unit': [new_geographic_unit],\n",
    "        'year': [year],\n",
    "        'year_zipcode': [year_zipcode],\n",
    "        'geometry_zipcode': [zipcode_geometry],\n",
    "        'geometry_county': [county_geometry],\n",
    "        'geometry_tract': [tract_geometry],\n",
    "        'geometry_BG': [bg_geometry],\n",
    "        'geometry_lat_long': [lat_long_geometry],\n",
    "        'geometry_state': [state_geometry],\n",
    "        'geometry_new_unit': [intersection_geometry],\n",
    "        'cbgInconsistent': [cbgInconsistent],\n",
    "        'tractInconsistent': [tractInconsistent],\n",
    "        'countyInconsistent': [countyInconsistent],\n",
    "        'stateInconsistent': [stateInconsistent],\n",
    "        'latlongInconsistent': [latlongInconsistent],\n",
    "        'zipcodeInconsistent': [zipcodeInconsistent],\n",
    "        'multipleInconsistent': [multipleInconsistent],\n",
    "        'singleInconsistent': [singleInconsistent]\n",
    "        })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58117069",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da1e28-961e-48b3-869d-83b145ae3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df1 = new_unit_df[['geometry_new_unit','new_geographic_unit']]\n",
    "\n",
    "new_unit_df1 = new_unit_df1.drop_duplicates()\n",
    "\n",
    "chunk_size = 40000  # adjust based on your system's capabilities\n",
    "chunks = [x for x in range(0, len(new_unit_df1), chunk_size)]\n",
    "\n",
    "for start in chunks:\n",
    "    end = start + chunk_size\n",
    "    temp_df = new_unit_df1.iloc[start:end].copy()\n",
    "    temp_df['geometry_new_unit'] = temp_df['geometry_new_unit'].apply(lambda geom: geom.wkt)\n",
    "    temp_df.to_parquet(f\"new_geographic_unit_geometry_{start}_{end}.parquet.gzip\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_unit_df1 = new_unit_df[['geometry_new_unit','new_geographic_unit']]\n",
    "\n",
    "new_unit_df1 = new_unit_df.drop_duplicates()\n",
    "\n",
    "chunk_size = 10000  # adjust based on your system's capabilities\n",
    "chunks = [x for x in range(0, len(new_unit_df1), chunk_size)]\n",
    "\n",
    "for start in chunks:\n",
    "    end = start + chunk_size\n",
    "    temp_df = new_unit_df1.iloc[start:end].copy()\n",
    "    temp_df['geometry_new_unit'] = temp_df['geometry_new_unit'].apply(lambda geom: geom.wkt)\n",
    "    temp_df['geometry_zipcode'] = temp_df['geometry_zipcode'].apply(lambda geom: geom.wkt)\n",
    "    temp_df['geometry_county'] = temp_df['geometry_county'].apply(lambda geom: geom.wkt)\n",
    "    temp_df['geometry_tract'] = temp_df['geometry_tract'].apply(lambda geom: geom.wkt)\n",
    "    temp_df['geometry_BG'] = temp_df['geometry_BG'].apply(lambda geom: geom.wkt)\n",
    "    temp_df['geometry_lat_long'] = temp_df['geometry_lat_long'].apply(lambda geom: geom.wkt)\n",
    "    temp_df['geometry_state'] = temp_df['geometry_state'].apply(lambda geom: geom.wkt)\n",
    "    temp_df.to_parquet(f\"C:/Users/jorda/Box/Flood Damage PredictionProject/new_geographic_unit_geometry_all_{start}_{end}.parquet.gzip\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbeb24-8f7c-4cbe-982a-b620f25241d0",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2953bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assignment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_unit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d182e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(assignment_counts.values())\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(new_unit_df, geometry='geometry_new_unit')\n",
    "\n",
    "# Set the current CRS to WGS 84\n",
    "gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "# Filter for only the continental US using bounding box constraints\n",
    "continental_US = gdf.cx[-125:-66, 24:49]\n",
    "\n",
    "# Project the filtered GeoDataFrame to Web Mercator (EPSG:3857)\n",
    "continental_US_projected = continental_US.to_crs(epsg=3857)\n",
    "\n",
    "# Compute the average area in square meters\n",
    "average_area_m2 = continental_US_projected.geometry.area.mean()\n",
    "median_area_m2 = continental_US_projected.geometry.area.median()\n",
    "\n",
    "# Convert the areas from square meters to square kilometers\n",
    "average_area_km2 = average_area_m2 / 1e6\n",
    "median_area_km2 = median_area_m2 / 1e6\n",
    "\n",
    "print(f\"Average Area: {average_area_km2} square kilometers\")\n",
    "print(f\"Median Area: {median_area_km2} square kilometers\")\n",
    "\n",
    "# Plot the geometries using the 'Reds' colormap\n",
    "continental_US_projected.plot(edgecolor=\"black\", cmap='Reds', figsize=(10, 10))\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Continental US: Using Latitude-Longitude geometries')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f825a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(new_unit_df, geometry='geometry_new_unit')\n",
    "\n",
    "# Set the CRS to WGS 84 if it's not already set\n",
    "gdf.crs = gdf.crs if gdf.crs else \"EPSG:4326\"\n",
    "\n",
    "# Project the GeoDataFrame to a metric CRS, e.g., Web Mercator (EPSG:3857)\n",
    "gdf_projected = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Calculate the areas in square meters\n",
    "gdf_projected['area_m2'] = gdf_projected['geometry_new_unit'].area\n",
    "\n",
    "# Sort the GeoDataFrame by the area, ascending\n",
    "gdf_sorted = gdf_projected.sort_values(by='area_m2', ascending=True)\n",
    "\n",
    "# Select the top 200 smallest areas\n",
    "top_200_smallest = gdf_sorted.head(100)\n",
    "\n",
    "# Convert the areas to square kilometers and print them\n",
    "areas_km2 = top_200_smallest['area_m2'] / 1e6\n",
    "\n",
    "for idx, area in enumerate(areas_km2, 1):\n",
    "    print(f\"Rank {idx}: {area:.70f} km^2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e69d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(new_unit_df, geometry='geometry_new_unit')\n",
    "\n",
    "# Set the CRS to WGS 84 if it's not already set\n",
    "gdf.crs = gdf.crs if gdf.crs else \"EPSG:4326\"\n",
    "\n",
    "# Filter rows where the area of 'geometry_new_unit' is 0 and 'geometry_lat_long' is not empty\n",
    "zero_area_with_latlong = gdf[(gdf.geometry.area == 0) & gdf['geometry_lat_long'].notnull()]\n",
    "\n",
    "# If there are any units with area 0, plot the combined geometries for the first unit\n",
    "if not zero_area_with_latlong.empty:\n",
    "    unit = zero_area_with_latlong.iloc[15]\n",
    "    \n",
    "    # Extracting the geometries for the first unit\n",
    "    geom_lat_long = unit['geometry_lat_long']\n",
    "    geom_BG = unit['geometry_BG']\n",
    "    geom_zipcode = unit['geometry_zipcode']\n",
    "    \n",
    "    # Combine the three geometries into one GeoSeries for plotting\n",
    "    combined_geometries = gpd.GeoSeries([geom_lat_long, geom_BG, geom_zipcode])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    combined_geometries.plot(ax=ax, color=[(1,0,0,0.05), (0,0,1,.1), (0,1,0,1)])\n",
    "    print(unit['new_geographic_unit'])\n",
    "\n",
    "    ax.set_title('Combined Geometries for Single Unit with Zero Area in geometry_new_unit')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No units with area of 0 in 'geometry_new_unit' with non-empty 'geometry_lat_long'.\")\n",
    "    \n",
    "# Check if there's an intersection between geom_lat_long and geom_zipcode\n",
    "intersection = geom_lat_long.intersection(geom_zipcode)\n",
    "\n",
    "if intersection.is_empty:\n",
    "    print(\"The intersection between lat/long and zipcode geometries is empty.\")\n",
    "else:\n",
    "    print(\"There's an intersection between lat/long and zipcode geometries.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aef3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(new_unit_df, geometry='geometry_new_unit')\n",
    "\n",
    "# Set the CRS to WGS 84 if it's not already set\n",
    "gdf.crs = gdf.crs if gdf.crs else \"EPSG:4326\"\n",
    "\n",
    "# Filter rows where the area of 'geometry_new_unit' is 0 and 'geometry_lat_long' is not empty\n",
    "zero_area_with_latlong = gdf[(gdf.geometry.area == 0) & gdf['geometry_lat_long'].notnull()]\n",
    "\n",
    "# If there are any units with area 0, plot the combined geometries for the first unit\n",
    "if not zero_area_with_latlong.empty:\n",
    "    unit = zero_area_with_latlong.iloc[6]\n",
    "    \n",
    "    # Extracting the geometries for the first unit\n",
    "    geom_lat_long = unit['geometry_lat_long']\n",
    "    geom_BG = unit['geometry_BG']\n",
    "    geom_zipcode = unit['geometry_zipcode']\n",
    "    \n",
    "    # Combine the three geometries into one GeoSeries for plotting\n",
    "    combined_geometries = gpd.GeoSeries([geom_zipcode])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    combined_geometries.plot(ax=ax, color=[(0,1,0,1)])\n",
    "    \n",
    "    ax.set_title('Combined Geometries for Single Unit with Zero Area in geometry_new_unit')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No units with area of 0 in 'geometry_new_unit' with non-empty 'geometry_lat_long'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ac970",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee30476",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['yearOfLoss_1990_2021'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "(gdf['geometry_lat_long'] == None).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda090b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_projected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1068bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the area of 'geometry_new_unit' is 0 and 'geometry_lat_long' is not empty\n",
    "zero_area_with_latlong = gdf[(gdf_projected['area_m2'] == 0) & (gdf_projected['geometry_lat_long']!= None)]\n",
    "\n",
    "# Print the 'year' column values for these ro\n",
    "zero_area_with_latlong['yearOfLoss_1990_2021'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows with an area of 0\n",
    "count_zero_area = (gdf_projected['area_m2'] == 0 & (gdf_projected['geometry_lat_long']!= None)).sum()\n",
    "\n",
    "print(f\"Number of observations with area of 0: {count_zero_area}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "6748/585007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73577916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f57f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute mean and median areas for a geometry column\n",
    "def compute_area_statistics(df, geometry_column):\n",
    "    # Convert the specific column to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry_column)\n",
    "    \n",
    "    # Set the current CRS to WGS 84\n",
    "    gdf.crs = \"EPSG:4326\"\n",
    "\n",
    "    # Project to a metric-based CRS (Web Mercator in this case, but this might introduce some area distortion)\n",
    "    gdf_projected = gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Filter out missing/empty geometries\n",
    "    gdf_projected = gdf_projected[gdf_projected[geometry_column].notna() & ~gdf_projected[geometry_column].is_empty]\n",
    "    \n",
    "    # Compute areas in square meters \n",
    "    gdf_projected['area'] = gdf_projected[geometry_column].area\n",
    "    \n",
    "    # Return mean and median\n",
    "    return gdf_projected['area'].mean() / 1e6, gdf_projected['area'].median() / 1e6  # Convert to km^2\n",
    "\n",
    "# Compute for each column\n",
    "for column in ['geometry_lat_long', 'geometry_BG']:\n",
    "    mean_area, median_area = compute_area_statistics(df_geographic_unique, column)\n",
    "    print(f\"Mean area for {column}: {mean_area} km^2\")\n",
    "    print(f\"Median area for {column}: {median_area} km^2\")\n",
    "    print(\"-------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid geometries\n",
    "gdf_zipcode = gdf_zipcode[gdf_zipcode['geometry_zipcode'].notna()]\n",
    "\n",
    "# Project each geometry one-by-one to avoid memory error\n",
    "def safe_transform(geom):\n",
    "    try:\n",
    "        return geom.to_crs(epsg=3857)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "gdf_zipcode['geometry_projected'] = gdf_zipcode['geometry_zipcode'].apply(safe_transform)\n",
    "\n",
    "# Now compute areas for the projected geometries\n",
    "gdf_zipcode['area_m2'] = gdf_zipcode['geometry_projected'].area\n",
    "\n",
    "# Count the number of rows with an area of 0\n",
    "count_zero_area = (gdf_zipcode['area_m2'] == 0).sum()\n",
    "\n",
    "print(f\"Number of observations in 'geometry_zipcode' with area of 0: {count_zero_area}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((new_unit_df['new_geographic_unit'].isnull()) | (new_unit_df['new_geographic_unit'] == ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68668e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts = new_unit_df['new_geographic_unit'].value_counts().sum()\n",
    "print(total_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a500a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_unit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geographic_unique['geometry_lat_long'].isnull().sum() + 297957\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2eca9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edde67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the ZIPcodes that have shape files for both 2020 and 2010\n",
    "unique_zipcodes_2020 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2020]['reportedZipCode'])\n",
    "unique_zipcodes_2010 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2010]['reportedZipCode'])\n",
    "common_zipcodes = unique_zipcodes_2020.intersection(unique_zipcodes_2010)\n",
    "\n",
    "# Initialize a DataFrame to store overlap information\n",
    "overlap_df = pd.DataFrame(columns=['ZIPcode', 'Overlap_Area', 'Original_Area_2010', 'Area_Difference_Change'])\n",
    "\n",
    "# Convert df_geographic_unique into a GeoDataFrame for geometry operations\n",
    "gdf = gpd.GeoDataFrame(df_geographic_unique, geometry='geometry_zipcode')\n",
    "\n",
    "for zipcode in common_zipcodes:\n",
    "    geometry_2020 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2020)].iloc[0]['geometry_zipcode']\n",
    "    geometry_2010 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2010)].iloc[0]['geometry_zipcode']\n",
    "\n",
    "    # Check if either geometry is None before proceeding\n",
    "    if geometry_2020 is None or geometry_2010 is None:\n",
    "        continue\n",
    "\n",
    "    original_area_2010 = geometry_2010.area\n",
    "    overlap_area = geometry_2010.intersection(geometry_2020).area\n",
    "    area_diff_change = overlap_area / original_area_2010 if original_area_2010 != 0 else 0  # Handle cases where original area is 0\n",
    "    \n",
    "    overlap_df.loc[len(overlap_df)] = {'ZIPcode': zipcode, \n",
    "                                       'Overlap_Area': overlap_area, \n",
    "                                       'Original_Area_2010': original_area_2010, \n",
    "                                       'Area_Difference_Change': area_diff_change}\n",
    "print(overlap_df[\"Area_Difference_Change\"].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2edb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the ZIPcodes that have shape files for both 2000 and 2010\n",
    "unique_zipcodes_2000 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2000]['reportedZipCode'])\n",
    "unique_zipcodes_2010 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2010]['reportedZipCode'])\n",
    "common_zipcodes = unique_zipcodes_2000.intersection(unique_zipcodes_2010)\n",
    "\n",
    "# Initialize a DataFrame to store overlap information\n",
    "overlap_df = pd.DataFrame(columns=['ZIPcode', 'Overlap_Area', 'Original_Area_2000', 'Area_Difference_Change'])\n",
    "\n",
    "# Convert df_geographic_unique into a GeoDataFrame for geometry operations\n",
    "gdf = gpd.GeoDataFrame(df_geographic_unique, geometry='geometry_zipcode')\n",
    "\n",
    "for zipcode in common_zipcodes:\n",
    "    geometry_2000 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2000)].iloc[0]['geometry_zipcode']\n",
    "    geometry_2010 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2010)].iloc[0]['geometry_zipcode']\n",
    "\n",
    "    # Check if either geometry is None before proceeding\n",
    "    if geometry_2000 is None or geometry_2010 is None:\n",
    "        continue\n",
    "\n",
    "    original_area_2000 = geometry_2000.area\n",
    "    overlap_area = geometry_2000.intersection(geometry_2010).area\n",
    "    area_diff_change = overlap_area / original_area_2000 if original_area_2000 != 0 else 0  # Handle cases where original area is 0\n",
    "    \n",
    "    overlap_df.loc[len(overlap_df)] = {'ZIPcode': zipcode, \n",
    "                                       'Overlap_Area': overlap_area, \n",
    "                                       'Original_Area_2000': original_area_2000, \n",
    "                                       'Area_Difference_Change': area_diff_change}\n",
    "print(overlap_df[\"Area_Difference_Change\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f51a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter out the ZIPcodes that have shape files for both 2000 and 2010\n",
    "unique_zipcodes_2000 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2000]['reportedZipCode'])\n",
    "unique_zipcodes_2010 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2010]['reportedZipCode'])\n",
    "common_zipcodes = unique_zipcodes_2000.intersection(unique_zipcodes_2010)\n",
    "\n",
    "# Initialize a DataFrame to store overlap information\n",
    "overlap_df = pd.DataFrame(columns=['ZIPcode', 'Overlap_Area', 'Original_Area_2000', 'Area_Difference_Change'])\n",
    "\n",
    "# Convert df_geographic_unique into a GeoDataFrame for geometry operations\n",
    "gdf = gpd.GeoDataFrame(df_geographic_unique, geometry='geometry_zipcode')\n",
    "\n",
    "tolerance = 0.001  # This is an arbitrary value and might need adjustingfor zipcode in common_zipcodes:\n",
    "for zipcode in common_zipcodes:\n",
    "\n",
    "    geometry_2000 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2000)].iloc[0]['geometry_zipcode']\n",
    "    geometry_2010 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2010)].iloc[0]['geometry_zipcode']\n",
    "\n",
    "    # Check if either geometry is None before proceeding\n",
    "    if geometry_2000 is None or geometry_2010 is None:\n",
    "        continue\n",
    "\n",
    "    # Simplify the geometry before buffering\n",
    "    simplified_geometry_2000 = geometry_2000.simplify(tolerance)\n",
    "    \n",
    "    original_area_2000 = simplified_geometry_2000.area\n",
    "    buffer_distance = (original_area_2000 * 0.10) ** 0.5  # Compute the buffer distance\n",
    "    buffered_geometry_2000 = simplified_geometry_2000.buffer(buffer_distance)\n",
    "\n",
    "    overlap_area = buffered_geometry_2000.intersection(geometry_2010).area\n",
    "    final_area_2010 = geometry_2010.area\n",
    "    overlap_to_final_ratio = overlap_area / final_area_2010 if final_area_2010 != 0 else 0  # Handle cases where final area is 0\n",
    "    \n",
    "    overlap_df.loc[len(overlap_df)] = {'ZIPcode': zipcode, \n",
    "                                       'Overlap_Area': overlap_area, \n",
    "                                       'Original_Area_2000': original_area_2000, \n",
    "                                       'Area_Difference_Change': overlap_to_final_ratio}  # Adjusted column name to reflect change\n",
    "\n",
    "print(overlap_df[\"Area_Difference_Change\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a2ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the ZIPcodes that have shape files for both 2000 and 2010\n",
    "unique_zipcodes_2000 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2000]['reportedZipCode'])\n",
    "unique_zipcodes_2010 = set(df_geographic_unique[df_geographic_unique['yearOfLoss_1990_2021'] == 2010]['reportedZipCode'])\n",
    "common_zipcodes = unique_zipcodes_2000.intersection(unique_zipcodes_2010)\n",
    "\n",
    "# Initialize a DataFrame to store overlap information\n",
    "overlap_df = pd.DataFrame(columns=['ZIPcode', 'Overlap_Area', 'Original_Area_2000', 'Area_Difference_Change'])\n",
    "\n",
    "# Convert df_geographic_unique into a GeoDataFrame for geometry operations\n",
    "gdf = gpd.GeoDataFrame(df_geographic_unique, geometry='geometry_zipcode')\n",
    "\n",
    "for zipcode in common_zipcodes:\n",
    "\n",
    "    geometry_2000 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2000)].iloc[0]['geometry_zipcode']\n",
    "    geometry_2010 = gdf[(gdf['reportedZipCode'] == zipcode) & (gdf['yearOfLoss_1990_2021'] == 2010)].iloc[0]['geometry_zipcode']\n",
    "\n",
    "    # Check if either geometry is None before proceeding\n",
    "    if geometry_2000 is None or geometry_2010 is None:\n",
    "        continue\n",
    "\n",
    "    original_area_2000 = geometry_2000.area\n",
    "    overlap_area = geometry_2000.intersection(geometry_2010).area\n",
    "    final_area_2010 = geometry_2010.area\n",
    "    overlap_to_final_ratio = overlap_area / final_area_2010 if final_area_2010 != 0 else 0  # Handle cases where final area is 0\n",
    "\n",
    "    overlap_df.loc[len(overlap_df)] = {'ZIPcode': zipcode, \n",
    "                                       'Overlap_Area': overlap_area, \n",
    "                                       'Original_Area_2000': original_area_2000, \n",
    "                                       'Area_Difference_Change': overlap_to_final_ratio}\n",
    "\n",
    "print(overlap_df[\"Area_Difference_Change\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906dbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(new_unit_df, geometry='geometry_new_unit')\n",
    "\n",
    "# Set the CRS to WGS 84 if it's not already set\n",
    "gdf.crs = gdf.crs if gdf.crs else \"EPSG:4326\"\n",
    "\n",
    "# Filter rows where the area of 'geometry_new_unit' is 0 and 'geometry_lat_long' is not empty\n",
    "zero_area_with_latlong = gdf[(gdf.geometry.area == 0) & gdf['geometry_lat_long'].notnull()]\n",
    "\n",
    "# Calculate the number of zero area values with non-empty zip geometry\n",
    "num_zero_area_with_latlong = len(zero_area_with_latlong)\n",
    "print(f\"Number of zero area values with non-empty 'geometry_lat_long': {num_zero_area_with_latlong}\")\n",
    "\n",
    "# If there are any units with area 0, plot the combined geometries for the first unit\n",
    "if not zero_area_with_latlong.empty:\n",
    "    unit = zero_area_with_latlong.iloc[0]\n",
    "    \n",
    "    # Extracting the geometries for the first unit\n",
    "    geom_lat_long = unit['geometry_lat_long']\n",
    "    geom_BG = unit['geometry_BG']\n",
    "    geom_zipcode = unit['geometry_zipcode']\n",
    "    \n",
    "    # Combine the three geometries into one GeoSeries for plotting\n",
    "    combined_geometries = gpd.GeoSeries([geom_lat_long, geom_BG, geom_zipcode])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    combined_geometries.plot(ax=ax, color=[(1,0,0,0.5), (0,0,1,0.9), (0,1,0,0.5)])\n",
    "    \n",
    "    ax.set_title('Combined Geometries for Single Unit with Zero Area in geometry_new_unit')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No units with area of 0 in 'geometry_new_unit' with non-empty 'geometry_lat_long'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(new_unit_df, geometry='geometry_new_unit')\n",
    "\n",
    "# Set the CRS to WGS 84 if it's not already set\n",
    "gdf.crs = gdf.crs if gdf.crs else \"EPSG:4326\"\n",
    "\n",
    "# Filter rows where the area of 'geometry_new_unit' is 0 and 'geometry_zipcode' is not empty\n",
    "zero_area_with_zip = gdf[(gdf.geometry.area == 0) & gdf['geometry_zipcode'].notnull()]\n",
    "\n",
    "# Calculate the number of zero area values with non-empty zip geometry\n",
    "num_zero_area_with_zip = len(zero_area_with_zip)\n",
    "print(f\"Number of zero area values with non-empty zip code: {num_zero_area_with_zip}\")\n",
    "\n",
    "# If there are any units with area 0, plot the combined geometries for the first unit\n",
    "if not zero_area_with_zip.empty:\n",
    "    unit = zero_area_with_zip.iloc[0]\n",
    "    \n",
    "    # Extracting the geometries for the first unit\n",
    "    geom_lat_long = unit['geometry_lat_long']\n",
    "    geom_BG = unit['geometry_BG']\n",
    "    geom_zipcode = unit['geometry_zipcode']\n",
    "    \n",
    "    # Combine the three geometries into one GeoSeries for plotting\n",
    "    combined_geometries = gpd.GeoSeries([geom_lat_long, geom_BG, geom_zipcode])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    combined_geometries.plot(ax=ax, color=[(1,0,0,0.5), (0,0,1,0.9), (0,1,0,0.5)])\n",
    "    \n",
    "    ax.set_title('Combined Geometries for Single Unit with Zero Area in geometry_new_unit')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No units with area of 0 in 'geometry_new_unit' with non-empty zip code.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(new_unit_df, geometry='geometry_new_unit')\n",
    "\n",
    "# Set the CRS to WGS 84 if it's not already set\n",
    "gdf.crs = gdf.crs if gdf.crs else \"EPSG:4326\"\n",
    "\n",
    "# Filter rows where the area of 'geometry_new_unit' is 0, 'geometry_lat_long' is not empty, and 'geometry_zipcode' is not empty\n",
    "zero_area_with_latlong_zip = gdf[(gdf.geometry.area == 0) & gdf['geometry_lat_long'].notnull() & gdf['geometry_zipcode'].notnull()]\n",
    "\n",
    "# Calculate the number of zero area values with non-empty lat-long and zip geometry\n",
    "num_zero_area_with_latlong_zip = len(zero_area_with_latlong_zip)\n",
    "print(f\"Number of zero area values with non-empty lat-long and zip code: {num_zero_area_with_latlong_zip}\")\n",
    "\n",
    "# If there are any such units, plot the combined geometries for the first unit\n",
    "if not zero_area_with_latlong_zip.empty:\n",
    "    unit = zero_area_with_latlong_zip.iloc[0]\n",
    "    \n",
    "    # Extracting the geometries for the first unit\n",
    "    geom_lat_long = unit['geometry_lat_long']\n",
    "    geom_BG = unit['geometry_BG']\n",
    "    geom_zipcode = unit['geometry_zipcode']\n",
    "    \n",
    "    # Combine the three geometries into one GeoSeries for plotting\n",
    "    combined_geometries = gpd.GeoSeries([geom_lat_long, geom_BG, geom_zipcode])\n",
    "    \n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    combined_geometries.plot(ax=ax, color=[(1,0,0,0.5), (0,0,1,0.9), (0,1,0,0.5)])\n",
    "    \n",
    "    ax.set_title('Combined Geometries for Single Unit with Zero Area in geometry_new_unit')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No units with area of 0 in 'geometry_new_unit' with non-empty lat-long and zip code.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame if it's not already one\n",
    "if not isinstance(df_geographic_unique, gpd.GeoDataFrame):\n",
    "    df_geographic_unique = gpd.GeoDataFrame(df_geographic_unique)\n",
    "\n",
    "# Filter rows where 'geometry_lat_long' is empty but 'geometry_zipcode' is not empty\n",
    "no_latlong_with_zip = df_geographic_unique[df_geographic_unique['geometry_lat_long'].isnull() & df_geographic_unique['geometry_zipcode'].notnull()]\n",
    "\n",
    "# Calculate the number of such observations\n",
    "num_no_latlong_with_zip = len(no_latlong_with_zip)\n",
    "print(f\"Number of observations without lat-long but with zip code: {num_no_latlong_with_zip}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f950b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_unit_df1 = new_unit_df1.drop_duplicates()\n",
    "\n",
    "chunk_size = 40000  # adjust based on your system's capabilities\n",
    "chunks = [x for x in range(0, len(new_unit_df1), chunk_size)]\n",
    "\n",
    "for start in chunks:\n",
    "    end = start + chunk_size\n",
    "    temp_df = new_unit_df1.iloc[start:end].copy()\n",
    "    temp_df['geometry'] = temp_df['geometry'].apply(lambda geom: geom.wkt)\n",
    "    temp_df.to_parquet(f\"new_geographic_unit_geometry_{start}_{end}.parquet.gzip\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c6790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fbb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe5e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "true = True\n",
    "if(true):\n",
    "    visualize = df_geographic_unique[df_geographic_unique['state'] == 'NC']\n",
    "\n",
    "    # Explicitly create GeoSeries from the 'geometry_BG' column\n",
    "    geometry_bg = gpd.GeoSeries(visualize['geometry_BG'])\n",
    "    geometry_zipcode = gpd.GeoSeries(visualize['geometry_zipcode'])\n",
    "\n",
    "    # Ensure that the GeoSeries has the correct Coordinate Reference System (CRS)\n",
    "    geometry_bg.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    geometry_zipcode.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    # Calculate the centroids for the filtered data\n",
    "    centroids = geometry_bg.centroid\n",
    "\n",
    "    # Apply the centroid-based filter to the DataFrame\n",
    "    visualize_NC_filtered = visualize[(centroids.y < 37) & (centroids.y > 22)]\n",
    "\n",
    "    # Create GeoDataFrames for block groups and zip codes using the filtered DataFrame\n",
    "    gdf_bg = gpd.GeoDataFrame(visualize_NC_filtered, geometry=geometry_bg)\n",
    "    gdf_zipcode = gpd.GeoDataFrame(visualize_NC_filtered, geometry=geometry_zipcode)\n",
    "\n",
    "    # Calculate the intersection of the two GeoDataFrames\n",
    "    gdf_intersection = gpd.overlay(gdf_bg, gdf_zipcode, how='intersection')\n",
    "\n",
    "    # Project the intersection to a suitable CRS\n",
    "    gdf_intersection_projected = gdf_intersection.to_crs(epsg=3857)\n",
    "\n",
    "    # If plotting dataset is still too large, consider sampling\n",
    "    if len(gdf_intersection_projected) > 50000:  # adjust this threshold based on your system's capability\n",
    "        gdf_intersection_projected = gdf_intersection_projected.sample(50000)\n",
    "\n",
    "    # Create a base plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    # Plot the boundaries of intersections\n",
    "    gdf_intersection_projected.boundary.plot(ax=ax, color='green', linewidth=2, label='Overlapping Boundaries')\n",
    "\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('Overlap between Block Groups and Zip Codes in NC')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Cleanup\n",
    "    del visualize, geometry_bg, geometry_zipcode, centroids, visualize_NC_filtered, gdf_bg, gdf_zipcode, gdf_intersection, gdf_intersection_projected\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d9261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc74c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8475d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf180b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25850e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ab345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f4016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2256e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bae01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693718e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
