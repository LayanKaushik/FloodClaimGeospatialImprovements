{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad173b4-f3cf-42ec-868a-9299a5f83f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_columns', None)\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pygris\n",
    "from shapely.geometry import Polygon\n",
    "import shapely\n",
    "from inconsistency_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c14cce-9b5c-4544-ba9a-54d8687d93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the original dataset\n",
    "\n",
    "df = pd.read_parquet(\"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/FimaNfipClaims.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdb0931-1b28-424e-b2c0-a8b52520b103",
   "metadata": {},
   "source": [
    "## Third Dataset [NA County]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee18a71-afc7-4f70-a6bd-ca5c4c514814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geographic_unique = df[['state', 'reportedZipCode', 'countyCode', 'censusTract', 'censusBlockGroupFips', 'latitude', 'longitude', 'yearOfLoss']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6e60c9-ea78-46fd-b26d-be55cc884bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining conditions\n",
    "condition1 = (df_geographic_unique['latitude'].notna() & \n",
    "              df_geographic_unique['censusBlockGroupFips'].notna() & \n",
    "              df_geographic_unique['countyCode'].isna() & \n",
    "              df_geographic_unique['reportedZipCode'].notna())\n",
    "\n",
    "# Filtering the dataframe based on the combined conditions\n",
    "df_geographic_unique = df_geographic_unique[condition1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a1f90d-12de-423c-8fb5-288d500d6c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>reportedZipCode</th>\n",
       "      <th>countyCode</th>\n",
       "      <th>censusTract</th>\n",
       "      <th>censusBlockGroupFips</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>yearOfLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21824</th>\n",
       "      <td>FL</td>\n",
       "      <td>25302.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.403900e+10</td>\n",
       "      <td>5.403900e+11</td>\n",
       "      <td>38.3</td>\n",
       "      <td>-81.6</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23323</th>\n",
       "      <td>VA</td>\n",
       "      <td>24112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.169000e+10</td>\n",
       "      <td>5.169000e+11</td>\n",
       "      <td>36.7</td>\n",
       "      <td>-79.8</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70859</th>\n",
       "      <td>AL</td>\n",
       "      <td>35766.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.212700e+10</td>\n",
       "      <td>7.212700e+11</td>\n",
       "      <td>18.4</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81466</th>\n",
       "      <td>CA</td>\n",
       "      <td>95901.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.011706e+10</td>\n",
       "      <td>2.011706e+11</td>\n",
       "      <td>39.8</td>\n",
       "      <td>-96.6</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92161</th>\n",
       "      <td>NC</td>\n",
       "      <td>28401.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.404795e+10</td>\n",
       "      <td>2.404795e+11</td>\n",
       "      <td>38.4</td>\n",
       "      <td>-75.1</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      state  reportedZipCode  countyCode   censusTract  censusBlockGroupFips  \\\n",
       "21824    FL          25302.0         NaN  5.403900e+10          5.403900e+11   \n",
       "23323    VA          24112.0         NaN  5.169000e+10          5.169000e+11   \n",
       "70859    AL          35766.0         NaN  7.212700e+10          7.212700e+11   \n",
       "81466    CA          95901.0         NaN  2.011706e+10          2.011706e+11   \n",
       "92161    NC          28401.0         NaN  2.404795e+10          2.404795e+11   \n",
       "\n",
       "       latitude  longitude  yearOfLoss  \n",
       "21824      38.3      -81.6        2004  \n",
       "23323      36.7      -79.8        2018  \n",
       "70859      18.4      -66.0        1990  \n",
       "81466      39.8      -96.6        1997  \n",
       "92161      38.4      -75.1        1996  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geographic_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e576ba-9ff0-4dee-ba31-d030076e845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "4964\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(df_geographic_unique['state'].isna()))\n",
    "print(sum(df_geographic_unique['reportedZipCode'].isna()))\n",
    "print(sum(df_geographic_unique['countyCode'].isna()))\n",
    "print(sum(df_geographic_unique['censusTract'].isna()))\n",
    "print(sum(df_geographic_unique['censusBlockGroupFips'].isna()))\n",
    "print(sum(df_geographic_unique['latitude'].isna()))\n",
    "print(sum(df_geographic_unique['yearOfLoss'].isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f69b28-71c3-4a7a-b7c1-3033d04f3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert units to string type to easily map\n",
    "\n",
    "columns_to_format = {\n",
    "    'reportedZipCode': 5,\n",
    "    'censusBlockGroupFips': 12,\n",
    "    'censusTract': 11\n",
    "}\n",
    "\n",
    "df_geographic_unique = format_geographic_units(df_geographic_unique, columns_to_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620212c5-d479-4e74-bf6f-6d012033b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating year bins to map (NEED TO UPDATE IT ONCE WE HAVE 2023 ZIPCODE SHAPEFILE DATA AND THE LATEST ORIGINAL FEMA DATASET) \n",
    "\n",
    "bins_1980_2021 = [df['yearOfLoss'].min(), 1980, 1990, 2000, 2010, 2020, df['yearOfLoss'].max() + 1]\n",
    "labels_1980_2021 = [0, 1980, 1990, 2000, 2010, 2020]\n",
    "\n",
    "custom_bins = [0, 2000] + list(range(2010, 2023)) + [2024]\n",
    "custom_labels = [0, 2000] + list(range(2010, 2023))\n",
    "\n",
    "df_geographic_unique = process_year_of_loss(df_geographic_unique, bins_1980_2021, labels_1980_2021, custom_bins, custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91755617-ba6d-425a-97ea-3eb2872dad1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>reportedZipCode</th>\n",
       "      <th>countyCode</th>\n",
       "      <th>censusTract</th>\n",
       "      <th>censusBlockGroupFips</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>yearOfLoss</th>\n",
       "      <th>yearOfLoss_1980_2021</th>\n",
       "      <th>zip_year_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21824</th>\n",
       "      <td>FL</td>\n",
       "      <td>25302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54039001100</td>\n",
       "      <td>540390011006</td>\n",
       "      <td>38.3</td>\n",
       "      <td>-81.6</td>\n",
       "      <td>2004</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23323</th>\n",
       "      <td>VA</td>\n",
       "      <td>24112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51690000500</td>\n",
       "      <td>516900005003</td>\n",
       "      <td>36.7</td>\n",
       "      <td>-79.8</td>\n",
       "      <td>2018</td>\n",
       "      <td>2010</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70859</th>\n",
       "      <td>AL</td>\n",
       "      <td>35766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72127004600</td>\n",
       "      <td>721270046004</td>\n",
       "      <td>18.4</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>1990</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81466</th>\n",
       "      <td>CA</td>\n",
       "      <td>95901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20117060510</td>\n",
       "      <td>201170605103</td>\n",
       "      <td>39.8</td>\n",
       "      <td>-96.6</td>\n",
       "      <td>1997</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92161</th>\n",
       "      <td>NC</td>\n",
       "      <td>28401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24047950300</td>\n",
       "      <td>240479503002</td>\n",
       "      <td>38.4</td>\n",
       "      <td>-75.1</td>\n",
       "      <td>1996</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      state reportedZipCode  countyCode  censusTract censusBlockGroupFips  \\\n",
       "21824    FL           25302         NaN  54039001100         540390011006   \n",
       "23323    VA           24112         NaN  51690000500         516900005003   \n",
       "70859    AL           35766         NaN  72127004600         721270046004   \n",
       "81466    CA           95901         NaN  20117060510         201170605103   \n",
       "92161    NC           28401         NaN  24047950300         240479503002   \n",
       "\n",
       "       latitude  longitude  yearOfLoss  yearOfLoss_1980_2021  zip_year_bin  \n",
       "21824      38.3      -81.6        2004                  2000          2000  \n",
       "23323      36.7      -79.8        2018                  2010          2018  \n",
       "70859      18.4      -66.0        1990                  1990             0  \n",
       "81466      39.8      -96.6        1997                  1990             0  \n",
       "92161      38.4      -75.1        1996                  1990             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geographic_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef5854-3e74-40d6-8a6a-eb2ed76be6b8",
   "metadata": {},
   "source": [
    "## Read shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15d219d-e1a7-4aec-99b0-27adeab37f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default year of 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Box\\Home Folder lk220\\Private\\Duke\\Duke Courses\\Research\\FloodDamagePrediction\\Code\\inconsistency_utils.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  state_df.rename(columns={'geometry': 'geometry_state'}, inplace=True)\n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 1289748480 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m units \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat_long\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzipcode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTract\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m df_shapefiles \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_rename_geographic_data_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m state_df \u001b[38;5;241m=\u001b[39m df_shapefiles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m lat_long_df \u001b[38;5;241m=\u001b[39m df_shapefiles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat_long\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\Box\\Home Folder lk220\\Private\\Duke\\Duke Courses\\Research\\FloodDamagePrediction\\Code\\inconsistency_utils.py:95\u001b[0m, in \u001b[0;36mload_and_rename_geographic_data_whole\u001b[1;34m(units, base_path)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[0;32m     94\u001b[0m     end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m chunk_size\n\u001b[1;32m---> 95\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mzipcode_geometry_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstart\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mend\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet.gzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     gdf_read \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame(temp_df, geometry\u001b[38;5;241m=\u001b[39mtemp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: shapely\u001b[38;5;241m.\u001b[39mwkt\u001b[38;5;241m.\u001b[39mloads(x)))\n\u001b[0;32m     97\u001b[0m     gdf_list\u001b[38;5;241m.\u001b[39mappend(gdf_read)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03mDataFrame\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    501\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py:251\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    245\u001b[0m     path,\n\u001b[0;32m    246\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    248\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    249\u001b[0m )\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    255\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_as_manager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\parquet\\core.py:2986\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2975\u001b[0m         \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2976\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   2977\u001b[0m             source, metadata\u001b[38;5;241m=\u001b[39mmetadata, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   2978\u001b[0m             memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2983\u001b[0m             thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   2984\u001b[0m         )\n\u001b[1;32m-> 2986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2987\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2989\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2990\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2991\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2992\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2993\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   2995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_prefixes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\parquet\\core.py:2614\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2606\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2607\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2608\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2609\u001b[0m         ]\n\u001b[0;32m   2610\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2611\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   2612\u001b[0m         )\n\u001b[1;32m-> 2614\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[0;32m   2617\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2619\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\_dataset.pyx:546\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\_dataset.pyx:3449\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\error.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 1289748480 failed"
     ]
    }
   ],
   "source": [
    "units = ['state', 'lat_long', 'BG', 'zipcode', 'Tract']\n",
    "base_path = \"C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/\"\n",
    "df_shapefiles = load_and_rename_geographic_data_whole(units, base_path)\n",
    "\n",
    "state_df = df_shapefiles['state']\n",
    "lat_long_df = df_shapefiles['lat_long']\n",
    "BG_df = df_shapefiles['BG']\n",
    "zipcode_df = df_shapefiles['zipcode']\n",
    "Tract_df = df_shapefiles['Tract']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d31ae-645f-481b-b2b7-5bf02e429ee6",
   "metadata": {},
   "source": [
    "## Geometry Intersection creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7d964-d50a-46fe-8aa9-ecc6fd7be9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for post 1980\n",
    "\n",
    "df_geographic_unique = df_geographic_unique[df_geographic_unique['yearOfLoss_1980_2021']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0f670-0dd5-45e7-929b-808f43a72fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the multi-index on lat_long_df\n",
    "lat_long_df.set_index(['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Mapping the values\n",
    "df_geographic_unique['geometry_lat_long'] = df_geographic_unique.set_index(['latitude', 'longitude']).index.map(lat_long_df['geometry_lat_long'])\n",
    "\n",
    "# Resetting the index of lat_long_df (return to multi-index)\n",
    "lat_long_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430f47c-6953-4e99-9ac5-daa8fef2f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "BG_df.set_index(['GEOID'], inplace=True)\n",
    "df_geographic_unique['geometry_BG'] = df_geographic_unique.set_index(['censusBlockGroupFips']).index.map(BG_df['geometry_BG'])\n",
    "\n",
    "BG_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4604f5-8f19-4c4f-a84f-de4d4e1cb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "zipcode_df.set_index(['ZIPcode', 'year'], inplace=True)\n",
    "df_geographic_unique['geometry_zipcode'] = df_geographic_unique.set_index(['reportedZipCode', 'zip_year_bin']).index.map(zipcode_df['geometry_zipcode'])\n",
    "\n",
    "zipcode_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098b61d-3487-44e1-b581-0ee7c32bf7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the multi-index on state_df\n",
    "state_df.set_index(['STUSPS'], inplace=True)\n",
    "\n",
    "# Mapping the values\n",
    "df_geographic_unique['geometry_state'] = df_geographic_unique.set_index(['state']).index.map(state_df['geometry_state'])\n",
    "\n",
    "state_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda65490-0b24-4515-935e-9ae0fae8d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial mapping with multi-index\n",
    "Tract_df.set_index(['censusTractID'], inplace=True)\n",
    "df_geographic_unique['geometry_tract'] = df_geographic_unique.set_index(['censusTract']).index.map(Tract_df['geometry_tract'])\n",
    "\n",
    "# Resetting the index of Tract_df (return to multi-index)\n",
    "Tract_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf7d38-1318-4e4b-ba42-7d0c5ee53dd4",
   "metadata": {},
   "source": [
    "#### Drop rows with missing shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1501752-d5da-42cf-a6c7-5bad119be754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geographic_unique = df_geographic_unique[(df_geographic_unique['geometry_BG'].notna())\n",
    "                              & (df_geographic_unique['geometry_zipcode'].notna())\n",
    "                              & (df_geographic_unique['geometry_tract'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e2fd7-95c9-4cfb-b7f6-c7a1c5486b4e",
   "metadata": {},
   "source": [
    "#### Creating the intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50f92b-0405-4ae2-8773-069c7d76d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty GeoDataFrame to store the intersection results\n",
    "new_unit_df = gpd.GeoDataFrame(columns=['reportedZipCode', 'countyCode', 'censusTract',\n",
    "                                       'censusBlockGroupFips', 'latitude', 'longitude', 'year', 'year_zipcode', 'state', 'geometry_zipcode',\n",
    "                                        'geometry_tract','geometry_BG','geometry_lat_long','geometry_state',\n",
    "                                         'cbgInconsistent', 'tractInconsistent', 'countyInconsistent', 'stateInconsistent', 'latlongInconsistent', 'multiple', 'noOverlap', 'oneWrong'])\n",
    "\n",
    "for idx_unit, row_unit in df_geographic_unique.iterrows():\n",
    "    year = row_unit['yearOfLoss_1980_2021']\n",
    "    year_zipcode = row_unit['zip_year_bin']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "    \n",
    "    # Compute intersection geometry\n",
    "    #intersection_geometry = bg_geometry.intersection(lat_long_geometry).intersection(zipcode_geometry).intersection(county_geometry).intersection(state_geometry).intersection(tract_geometry)\n",
    "    \n",
    "    # First intersection\n",
    "    intersection_1 = bg_geometry.intersection(lat_long_geometry)\n",
    "\n",
    "    # Second intersection\n",
    "    intersection_2 = intersection_1.intersection(zipcode_geometry)\n",
    "\n",
    "    \n",
    "    # Third intersection\n",
    "    # Fourth intersection\n",
    "    intersection_4 = intersection_2.intersection(tract_geometry)\n",
    "\n",
    "    # Fifth intersection\n",
    "    intersection_geometry = intersection_4.intersection(state_geometry)\n",
    "    if intersection_geometry is None or intersection_geometry.is_empty:\n",
    "        new_unit_df = pd.concat([new_unit_df, pd.DataFrame({\n",
    "            'reportedZipCode': [zipcode],\n",
    "            'countyCode': [county_id],\n",
    "            'censusTract': [tract_id],\n",
    "            'censusBlockGroupFips': [bg_id],\n",
    "            'latitude': [lat],\n",
    "            'longitude': [long],\n",
    "            'year': [year],\n",
    "            'year_zipcode': [year_zipcode],\n",
    "            'state': [state],\n",
    "            'geometry_zipcode': [zipcode_geometry],\n",
    "            'geometry_tract': [tract_geometry],\n",
    "            'geometry_BG': [bg_geometry],\n",
    "            'geometry_lat_long': [lat_long_geometry],\n",
    "            'geometry_state': [state_geometry]\n",
    "        })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd630f07-6f35-4f94-bcda-c4462527960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_unit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc12bc6-7efa-433b-abbb-1e3962388beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_unit, row_unit in new_unit_df.iterrows():\n",
    "    year = row_unit['year']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "    \n",
    "\n",
    "    intersection_geometry = lat_long_geometry.intersection(tract_geometry).intersection(zipcode_geometry).intersection(state_geometry)\n",
    "\n",
    "    if intersection_geometry is None or intersection_geometry.is_empty:\n",
    "        new_unit_df.at[idx_unit, 'cbgInconsistent'] = 0\n",
    "    else:\n",
    "        new_unit_df.at[idx_unit, 'cbgInconsistent'] = 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae71aad-6d95-4864-be4d-bc75c7d4e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df['cbgInconsistent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203053b-3f8e-427f-980b-e6137abd9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_unit, row_unit in new_unit_df.iterrows():\n",
    "    year = row_unit['year']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "    \n",
    "    intersection_geometry = bg_geometry.intersection(zipcode_geometry).intersection(lat_long_geometry).intersection(state_geometry)\n",
    "\n",
    "    if intersection_geometry is None or intersection_geometry.is_empty:\n",
    "        new_unit_df.at[idx_unit,'tractInconsistent'] = 0\n",
    "    else:\n",
    "        new_unit_df.at[idx_unit,'tractInconsistent'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a5b7f-8567-41e7-a548-02d9988fa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df['tractInconsistent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e08123-026c-4cdc-a8bc-dbd3fba0acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_unit, row_unit in new_unit_df.iterrows():\n",
    "    year = row_unit['year']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "    \n",
    "    intersection_geometry = bg_geometry.intersection(lat_long_geometry).intersection(zipcode_geometry).intersection(tract_geometry).intersection(state_geometry)\n",
    "\n",
    "    if intersection_geometry is None or intersection_geometry.is_empty:\n",
    "        new_unit_df.at[idx_unit,'countyInconsistent'] = 0\n",
    "    else:\n",
    "        new_unit_df.at[idx_unit,'countyInconsistent'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23a1d0-fcc2-4213-a98e-9dfe91ed6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df['countyInconsistent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c39d32-752c-40cd-8e49-2b28a63a043d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_unit, row_unit in new_unit_df.iterrows():\n",
    "    year = row_unit['year']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "\n",
    "    intersection_geometry = bg_geometry.intersection(zipcode_geometry).intersection(tract_geometry).intersection(lat_long_geometry)\n",
    "\n",
    "    if intersection_geometry is None or intersection_geometry.is_empty:\n",
    "        new_unit_df.at[idx_unit,'stateInconsistent'] = 0\n",
    "    else:\n",
    "        new_unit_df.at[idx_unit,'stateInconsistent'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e6c91-0654-4eb2-8142-4fea953b37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df['stateInconsistent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1df29-6553-442b-991a-ea573d3b826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_unit, row_unit in new_unit_df.iterrows():\n",
    "    year = row_unit['year']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "    \n",
    "    intersection_geometry = bg_geometry.intersection(zipcode_geometry).intersection(tract_geometry).intersection(state_geometry)\n",
    "\n",
    "    if intersection_geometry is None or intersection_geometry.is_empty:\n",
    "        new_unit_df.at[idx_unit,'latlongInconsistent'] = 0\n",
    "    else:\n",
    "        new_unit_df.at[idx_unit,'latlongInconsistent'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b145b4-38a2-481e-9356-be726e67e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df['latlongInconsistent'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65ff68-0b64-4b8f-8319-624786f5e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_unit, row_unit in new_unit_df.iterrows():\n",
    "    year = row_unit['year']\n",
    "    bg_id = row_unit['censusBlockGroupFips']\n",
    "    bg_geometry = row_unit['geometry_BG']\n",
    "    tract_id = row_unit['censusTract']\n",
    "    tract_geometry = row_unit['geometry_tract']\n",
    "    county_id = row_unit['countyCode']\n",
    "    state = row_unit['state']\n",
    "    state_geometry = row_unit['geometry_state']\n",
    "    lat_long_geometry = row_unit['geometry_lat_long']\n",
    "    lat = row_unit['latitude']\n",
    "    long = row_unit['longitude']\n",
    "    zipcode_geometry = row_unit['geometry_zipcode']\n",
    "    zipcode = row_unit['reportedZipCode']\n",
    "    \n",
    "    intersection_geometry = bg_geometry.intersection(lat_long_geometry).intersection(tract_geometry).intersection(state_geometry)\n",
    "\n",
    "    if intersection_geometry is None or intersection_geometry.is_empty:\n",
    "        new_unit_df.at[idx_unit,'zipInconsistent'] = 0\n",
    "    else:\n",
    "        new_unit_df.at[idx_unit,'zipInconsistent'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19496b-7ee4-4ccd-9c38-728801fc6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_unit_df['zipInconsistent'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34360fac-3daa-4c7e-9c87-fc0cdf6c1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (new_unit_df['latlongInconsistent'] +  new_unit_df['zipInconsistent']+ new_unit_df['stateInconsistent'] + new_unit_df['countyInconsistent'] + new_unit_df['tractInconsistent'] + new_unit_df['cbgInconsistent']) > 1\n",
    "new_unit_df.loc[mask, 'multiple'] = 1\n",
    "new_unit_df.loc[~mask, 'multiple'] = 0\n",
    "\n",
    "print(sum(new_unit_df['multiple']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d565a-bdfe-4e8a-88ef-c46dff14f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (new_unit_df['latlongInconsistent'] + \n",
    "             new_unit_df['stateInconsistent'] + \n",
    "             new_unit_df['countyInconsistent'] + \n",
    "             new_unit_df['tractInconsistent'] + \n",
    "             new_unit_df['cbgInconsistent'] +\n",
    "             new_unit_df['zipInconsistent']) == 0\n",
    "\n",
    "new_unit_df['noOverlap'] = 0  # Default to 0\n",
    "new_unit_df.loc[condition, 'noOverlap'] = 1\n",
    "\n",
    "print(sum(new_unit_df['noOverlap']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890f74b-3ea7-43a6-a8ab-c3892dadedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (new_unit_df['latlongInconsistent'] + \n",
    "             new_unit_df['stateInconsistent'] + \n",
    "             new_unit_df['countyInconsistent'] + \n",
    "             new_unit_df['tractInconsistent'] + \n",
    "             new_unit_df['cbgInconsistent'] +\n",
    "             new_unit_df['zipInconsistent']) == 1\n",
    "\n",
    "new_unit_df['oneWrong'] = 0  # Default to 0\n",
    "new_unit_df.loc[condition, 'oneWrong'] = 1\n",
    "\n",
    "print(sum(new_unit_df['oneWrong']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9530b6-9368-46f4-b7a4-76fc1bc9188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count dictionary for 'Wrong' values\n",
    "wrong_counts = {\n",
    "    'lat_long_geometry': 0,\n",
    "    'state_geometry': 0,\n",
    "    'county_geometry': 0,\n",
    "    'tract_geometry': 0,\n",
    "    'bg_geometry': 0,\n",
    "    'zip_geometry': 0  # added for zip codes\n",
    "}\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for idx_unit, row_unit in new_unit_df.iterrows():\n",
    "    if row_unit['oneWrong'] == 1:\n",
    "        if row_unit['latlongInconsistent'] == 1:\n",
    "            new_unit_df.at[idx_unit, 'lat_long_geometry'] = 'Wrong'\n",
    "            wrong_counts['lat_long_geometry'] += 1\n",
    "        if row_unit['stateInconsistent'] == 1:\n",
    "            new_unit_df.at[idx_unit, 'state_geometry'] = 'Wrong'\n",
    "            wrong_counts['state_geometry'] += 1\n",
    "        if row_unit['tractInconsistent'] == 1:\n",
    "            new_unit_df.at[idx_unit, 'tract_geometry'] = 'Wrong'\n",
    "            wrong_counts['tract_geometry'] += 1\n",
    "        if row_unit['cbgInconsistent'] == 1:\n",
    "            new_unit_df.at[idx_unit, 'bg_geometry'] = 'Wrong'\n",
    "            wrong_counts['bg_geometry'] += 1\n",
    "        if row_unit['zipInconsistent'] == 1:  # added for zip codes\n",
    "            new_unit_df.at[idx_unit, 'zip_geometry'] = 'Wrong'\n",
    "            wrong_counts['zip_geometry'] += 1\n",
    "\n",
    "# Print the 'Wrong' counts\n",
    "for category, count in wrong_counts.items():\n",
    "    print(f\"{category}: {count} 'Wrong' values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f12ea-67e3-458b-a2b4-6039b619622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unit_df['CBG_Zip_Mutually_Inconsistent'] = np.where(\n",
    "    (new_unit_df['latlongInconsistent'] == 0) & \n",
    "    (new_unit_df['cbgInconsistent'] == 1) & \n",
    "    (new_unit_df['zipInconsistent'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692bbf8-1eab-43b7-81e1-3c08814a9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(new_unit_df['CBG_Zip_Mutually_Inconsistent'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e26b4-0203-47ea-91df-fc463abf4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_unit_df[(new_unit_df['latlongInconsistent'] == 1) &(new_unit_df['cbgInconsistent'] == 1) & ((new_unit_df['latlongInconsistent'] + \n",
    "             new_unit_df['stateInconsistent'] + \n",
    "             new_unit_df['countyInconsistent'] + \n",
    "             new_unit_df['tractInconsistent'] + \n",
    "             new_unit_df['cbgInconsistent'] +\n",
    "             new_unit_df['zipInconsistent'])  == 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662ea0c-13ef-4027-8b4f-6c0807795042",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'geometry_tract', 'geometry_BG', 'geometry_zipcode', 'geometry_state', 'geometry_lat_long'\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "new_unit_df = new_unit_df.drop(columns= columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f83a1-120a-428d-a940-e0d7bd6c7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'zipInconsistent' column to integer type.\n",
    "new_unit_df['zipInconsistent'] = new_unit_df['zipInconsistent'].astype(int)\n",
    "\n",
    "new_unit_df= new_unit_df.drop_duplicates()\n",
    "\n",
    "new_unit_df.to_parquet('C:/Users/Asus/Box/Flood Damage PredictionProject/Dataset/inconsistency_dataframe_3.parquet.gzip', compression = 'gzip',  index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
